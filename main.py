import csv
import hashlib
import json
import math
import os
import re
import sys
import time
from datetime import datetime
from urllib.parse import urlencode

import pandas as pd
import requests
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- å…¨å±€é…ç½® ---
# ä»ç¯å¢ƒå˜é‡ä¸­è·å–Keyå’ŒSK
# è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå¹¶å¡«å…¥ä»¥ä¸‹å†…å®¹:
# TENCENT_MAP_KEY="ä½ çš„Key"
# TENCENT_MAP_SK="ä½ çš„Secret Key"
MY_KEY = os.getenv("TENCENT_MAP_KEY")
MY_SK = os.getenv("TENCENT_MAP_SK")


# APIç›¸å…³é…ç½®
API_HOST = "https://apis.map.qq.com"
API_PATH = "/ws/place/v1/suggestion"
PAGE_SIZE = 20
MAX_RETRIES = 10  # QPSè¶…é™æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2  # é‡è¯•å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# è¾“å…¥æ–‡ä»¶é…ç½®
EXCEL_FILE = "univ_moe.xls"
SUPP_JSON_FILE = "univ_supp.json"

# è¾“å‡ºæ–‡ä»¶é…ç½®
OUTPUT_JSON_FILE = "universities.json"
REJECTED_CSV_FILE = "rejected_pois.csv"
NO_DETAILS_CSV_FILE = "universities_without_details.csv"
NO_CAMPUSES_CSV_FILE = "universities_with_no_campuses.csv"
LOG_FILE_PREFIX = "run_log"


class QuotaExceededError(Exception):
    """å½“APIæ¯æ—¥è°ƒç”¨é‡è¾¾åˆ°ä¸Šé™æ—¶æŠ›å‡ºçš„è‡ªå®šä¹‰å¼‚å¸¸ã€‚"""

    pass


class Logger(object):
    """
    å°†æ§åˆ¶å°è¾“å‡ºåŒæ—¶å†™å…¥æ–‡ä»¶ã€‚
    """

    def __init__(self, filename="default.log"):
        self.terminal = sys.stdout
        # ä½¿ç”¨ 'w' æ¨¡å¼ï¼Œæ¯æ¬¡è¿è¡Œåˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        # è¿™ä¸ª flush æ–¹æ³•æ˜¯ä¸º Python 3 å…¼å®¹æ€§æ‰€å¿…éœ€çš„ã€‚
        pass


def request_tencent_api(path: str, params: dict, sk: str):
    """
    ä¸€ä¸ªç”¨äºè¯·æ±‚è…¾è®¯åœ°å›¾Web APIçš„ä»£ç†å‡½æ•°ï¼Œå¯è‡ªåŠ¨å¤„ç†ç­¾åå’ŒQPSè¶…é™é‡è¯•ã€‚
    """
    for attempt in range(MAX_RETRIES):
        # ç­¾åè®¡ç®—
        sorted_params = sorted(params.items())
        qs_for_sig = "&".join([f"{k}={v}" for k, v in sorted_params])
        string_to_sign = f"{path}?{qs_for_sig}{sk}"
        sig = hashlib.md5(string_to_sign.encode("utf-8")).hexdigest()

        # æ„é€ æœ€ç»ˆè¯·æ±‚URL
        encoded_params = urlencode(params)
        final_url = f"{API_HOST}{path}?{encoded_params}&sig={sig}"

        try:
            response = requests.get(final_url, timeout=15)
            response.raise_for_status()
            response_data = response.json()

            # æ£€æŸ¥APIè¿”å›çš„çŠ¶æ€ç 
            status = response_data.get("status")
            if status == 0:
                return response_data  # è¯·æ±‚æˆåŠŸ
            elif status == 120:
                print(
                    f"  - è§¦å‘QPSé™åˆ¶ (status 120)ï¼Œå°†åœ¨ {RETRY_DELAY} ç§’åé‡è¯•... (ç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡)"
                )
                time.sleep(RETRY_DELAY)
                continue  # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
            else:
                message = response_data.get("message")
                print(f"  - APIè¿”å›é”™è¯¯: status={status}, message={message}")
                if status == 121:
                    raise QuotaExceededError(message)
                return None

        except requests.exceptions.Timeout:
            print(f"  - è¯·æ±‚è¶…æ—¶: {final_url}")
        except requests.exceptions.HTTPError as e:
            print(f"  - HTTPé”™è¯¯: {e.response.status_code} for url: {final_url}")
        except requests.exceptions.RequestException as e:
            print(f"  - è¯·æ±‚å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

        # å¦‚æœä¸æ˜¯å› ä¸ºQPSé™åˆ¶å¯¼è‡´çš„å¤±è´¥ï¼Œåˆ™ç¨ä½œç­‰å¾…åé‡è¯•
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY)

    print(f"  - é‡è¯• {MAX_RETRIES} æ¬¡åä»ç„¶å¤±è´¥ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚")
    return None


def post_process_name(name: str) -> str:
    """å¯¹æ ¡åŒºåç§°è¿›è¡Œåå¤„ç†"""
    if not name:
        return name

    # 1. trim
    processed_name = name.strip()
    # 2. å»é™¤â€œ-â€å’Œâ€œ&â€
    processed_name = processed_name.replace("-", "").replace("&", "")
    # 3. å»é™¤ç»“å°¾çš„ç‰¹å®šæ–¹ä½å’ŒæœŸæ•°å­—æ ·
    suffixes_to_remove = [
        "è¥¿åŒº",
        "ä¸œåŒº",
        "åŒ—åŒº",
        "å—åŒº",
        "ä¸­åŒº",
        "ä¸€æœŸ",
        "äºŒæœŸ",
        "ä¸‰æœŸ",
        "å››æœŸ",
        "äº”æœŸ",
        "å…­æœŸ",
        "ä¸ƒæœŸ",
        "å…«æœŸ",
        "ä¹æœŸ",
        "åæœŸ",
    ]
    for suffix in suffixes_to_remove:
        if processed_name.endswith(suffix):
            processed_name = processed_name[: -len(suffix)]
            break  # åªç§»é™¤ä¸€ä¸ª
    # 4. å°†â€œXXä¸»æ ¡åŒºYYâ€æ›¿æ¢ä¸ºâ€œXXæ ¡åŒºYYâ€
    processed_name = re.sub(r"(.+)ä¸»æ ¡åŒº", r"\1æ ¡åŒº", processed_name)

    return processed_name.strip()


def is_valid_campus_name(name: str) -> bool:
    """æ£€æŸ¥åç§°æ˜¯å¦ç¬¦åˆæ ¡åŒºåå®šä¹‰"""
    if name == "":
        return True
    if not name:
        return False
    # éé™„å±
    if "é™„å±" in name or "åŒ»é™¢" in name:
        return False
    # ä»¥ç‰¹å®šè¯ç»“å°¾
    valid_endings = ["æ ¡åŒº", "å›­åŒº", "é™¢åŒº", "æ ¡å›­", "å­¦æ ¡", "åˆ†æ ¡", "é™¢"]
    for ending in valid_endings:
        if name.endswith(ending):
            return True
    return False


def check_and_get_name(part: str, poi_address: str):
    """
    è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥ä¸€ä¸ªéƒ¨åˆ†æ˜¯å¦æ˜¯æœ‰æ•ˆæ ¡åŒºåï¼Œæˆ–èƒ½å¦é€šè¿‡åœ°å€è¾…åŠ©æˆä¸ºæ ¡åŒºå
    """
    processed_part = post_process_name(part)
    if is_valid_campus_name(processed_part):
        return processed_part

    # å¦‚æœåç§°æ˜¯åœ°å€çš„å­ä¸²ï¼Œåˆ™åŠ ä¸Šâ€œæ ¡åŒºâ€
    if processed_part and poi_address and processed_part in poi_address:
        return f"{processed_part}æ ¡åŒº"

    return None


def parse_campus_name(poi: dict, school_name: str):
    """
    æ ¹æ®è§„åˆ™è§£æPOIæ ‡é¢˜ä»¥æå–æ ¡åŒºåç§°ã€‚
    è¿”å›åå¤„ç†è¿‡çš„æ ¡åŒºåï¼Œå¦‚æœæ— åŒ¹é…åˆ™è¿”å›Noneã€‚
    """
    poi_title = poi.get("title", "")
    poi_address = poi.get("address", "")

    # é€šè¿‡çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å­¦æ ¡åï¼Œå¿½ç•¥æ‹¬å·å·®å¼‚
    pattern_str = (
        re.escape(school_name).replace(r"ï¼ˆ", r"[\(ï¼ˆ]").replace(r"ï¼‰", r"[\)ï¼‰]")
    )
    pattern = re.compile(f"^{pattern_str}")
    match = pattern.match(poi_title)

    # å¦‚æœå®Œå…¨åŒ¹é…ï¼ˆåŒ…æ‹¬æ‹¬å·å·®å¼‚ï¼‰ï¼Œä¹Ÿè§†ä¸ºnullæ ¡åŒº
    if match and match.end() == len(poi_title):
        return None

    if not match:
        return "REJECT"

    remaining_title = poi_title[match.end() :].strip()
    if not remaining_title:
        return None

    if remaining_title in ["ä¸»æ ¡åŒº", "æ ¡æœ¬éƒ¨"]:
        return None

    # æ‹¬å·è§£æ B_0(B_1)...(B_n)
    parts_in_parentheses = re.findall(r"\(([^)]+)\)", remaining_title)
    # ä»åå¾€å‰æ£€æŸ¥æ‹¬å·å†…å®¹ (B_n -> B_1)
    for part in reversed(parts_in_parentheses):
        campus_name = check_and_get_name(part, poi_address)
        if campus_name is not None:
            return campus_name

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ‹¬å·å‰çš„å†…å®¹
    b0_part = remaining_title.split("(", 1)[0].strip()
    if b0_part:
        campus_name = check_and_get_name(b0_part, poi_address)
        if campus_name is not None:
            return campus_name

    # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œæ£€æŸ¥æ•´ä¸ªå‰©ä½™éƒ¨åˆ†
    if not parts_in_parentheses:
        campus_name = check_and_get_name(remaining_title, poi_address)
        if campus_name is not None:
            return campus_name

    return "REJECT"


def process_university_data(excel_path: str):
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œä»è¯»å–æ–‡ä»¶ã€åˆå¹¶æ•°æ®ã€APIè¯·æ±‚åˆ°è¾“å‡ºç»“æœçš„å®Œæ•´æµç¨‹ã€‚
    """
    if not MY_KEY or not MY_SK:
        print("é”™è¯¯ï¼šæ— æ³•ä» .env æ–‡ä»¶ä¸­åŠ è½½ TENCENT_MAP_KEY æˆ– TENCENT_MAP_SKã€‚")
        print("è¯·ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•ä¸‹å­˜åœ¨ .env æ–‡ä»¶ï¼Œå¹¶ä¸”å…¶ä¸­åŒ…å«æ­£ç¡®çš„Keyå’ŒSKã€‚")
        return

    # 1. è¯»å–Excelæ–‡ä»¶
    try:
        print(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {excel_path}")
        df_temp = pd.read_excel(excel_path, header=None)
        header_row_index = -1
        for i, row in df_temp.iterrows():
            if not isinstance(i, int):
                continue
            if "å­¦æ ¡åç§°" in str(row.values):
                header_row_index = i
                break
        if header_row_index == -1:
            print("é”™è¯¯ï¼šåœ¨Excelæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åŒ…å«â€œå­¦æ ¡åç§°â€çš„è¡¨å¤´è¡Œã€‚")
            return
        print(f"æ£€æµ‹åˆ°è¡¨å¤´åœ¨ç¬¬ {header_row_index + 1} è¡Œã€‚")
        df = pd.read_excel(excel_path, header=header_row_index)
    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 2. åŠ è½½ç”¨äºåˆå¹¶çš„é™„åŠ JSONæ•°æ®
    supp_data_map = {}
    try:
        print(f"æ­£åœ¨åŠ è½½é™„åŠ æ•°æ®æ–‡ä»¶: {SUPP_JSON_FILE}")
        with open(SUPP_JSON_FILE, "r", encoding="utf-8") as f:
            supp_list = json.load(f)
            supp_data_map = {item["name"]: item for item in supp_list}
        print(f"æˆåŠŸåŠ è½½ {len(supp_data_map)} æ¡é™„åŠ æ•°æ®ç”¨äºåˆå¹¶ã€‚")
    except Exception as e:
        print(f"åŠ è½½ '{SUPP_JSON_FILE}' æ—¶å‡ºé”™: {e}ï¼Œå°†è·³è¿‡æ•°æ®åˆå¹¶æ­¥éª¤ã€‚")

    # 3. æ¸…ç†å’Œé‡å‘½ååˆ—
    column_mapping = {
        "å­¦æ ¡æ ‡è¯†ç ": "id",
        "å­¦æ ¡åç§°": "name",
        "ä¸»ç®¡éƒ¨é—¨": "affiliation",
        "åŠå­¦å±‚æ¬¡": "type",
    }
    # åªä¿ç•™éœ€è¦çš„åˆ—
    df = df[list(column_mapping.keys())].rename(columns=column_mapping)
    df.dropna(subset=["name"], inplace=True)

    # æ¸…ç†IDåˆ—ï¼Œç¡®ä¿ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„æ•´æ•°
    df.dropna(subset=["id"], inplace=True)
    df["id"] = df["id"].astype(float).astype(int).astype(str)

    # æ¸…ç†å­¦æ ¡åç§°
    def clean_name(name):
        if isinstance(name, str):
            if name.startswith("æ°‘åŠ"):
                name = name[2:]
            # å»æ‰â€œå¸‚â€ï¼Œä½†ä¸å»æ‰â€œåŸå¸‚â€æˆ–â€œéƒ½å¸‚â€
            name = re.sub(r"(?<![åŸéƒ½])å¸‚", "", name)
        return name

    df["name"] = df["name"].apply(clean_name)

    # åˆå§‹åŒ–ç»“æœå’ŒæŠ¥å‘Šåˆ—è¡¨
    universities_list = df.to_dict("records")
    rejected_pois = []
    final_universities_data = []
    schools_without_details = []
    processed_poi_ids = set()
    quota_exceeded = False  # ç”¨äºæ ‡è®°é…é¢æ˜¯å¦è€—å°½

    print(f"æˆåŠŸè¯»å–å¹¶æ¸…ç†äº† {len(universities_list)} æ‰€å­¦æ ¡ã€‚å¼€å§‹å¤„ç†...")

    try:  # åŒ…è£¹ä¸»å¾ªç¯ä»¥ä¾¿æ•è·é…é¢å¼‚å¸¸
        # 4. éå†æ¯æ‰€å­¦æ ¡
        for index, school_from_xls in enumerate(universities_list):
            school_name = school_from_xls.get("name")
            if not school_name:
                print(
                    f"\n[{index + 1}/{len(universities_list)}] å­¦æ ¡åç§°ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ¡è®°å½•ã€‚"
                )
                continue
            print(f"\n[{index + 1}/{len(universities_list)}] æ­£åœ¨æŸ¥è¯¢: {school_name}")

            school_output = {
                "id": school_from_xls.get("id"),
                "name": school_name,
                "affiliation": school_from_xls.get("affiliation"),
                "type": school_from_xls.get("type"),
            }

            # åˆå¹¶é™„åŠ æ•°æ®
            fields_to_merge = [
                "majorCategory",
                "natureOfRunning",
                "is985",
                "is211",
                "isDoubleFirstClass",
            ]
            if school_name in supp_data_map:
                details = supp_data_map[school_name]
                for field in fields_to_merge:
                    if field in details:
                        school_output[field] = details[field]
            else:
                print(f"  - åœ¨ {SUPP_JSON_FILE} ä¸­æœªæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                for field in fields_to_merge:
                    school_output[field] = None
                schools_without_details.append(school_from_xls)

            school_output["campuses"] = []
            processed_campus_names = set()  # ç”¨äºæ ¡åŒºå»é‡
            page_index = 1
            total_pages = 1

            # å¤„ç†åˆ†é¡µAPIè¯·æ±‚
            while page_index <= total_pages:
                print(f"  - æ­£åœ¨è¯·æ±‚ç¬¬ {page_index}/{total_pages} é¡µ...")
                params = {
                    "keyword": school_name,
                    "key": MY_KEY,
                    "filter": "category=å¤§å­¦",
                    "get_ad": 1,
                    "page_size": PAGE_SIZE,
                    "page_index": page_index,
                    "added_fields": "category_code",
                }
                response_data = request_tencent_api(API_PATH, params, MY_SK)
                time.sleep(0.2)

                if response_data:
                    if page_index == 1:
                        count = response_data.get("count", 0)
                        total_pages = math.ceil(count / PAGE_SIZE)

                    for poi in response_data.get("data", []):
                        poi_id = poi.get("id")

                        if poi_id and poi_id in processed_poi_ids:
                            print(
                                f"    [ğŸŒ] {poi.get('title')} (ID: {poi_id} å·²åœ¨å…¨å±€ä¿å­˜)"
                            )
                            continue

                        campus_name_processed = parse_campus_name(poi, school_name)
                        if campus_name_processed == "":
                            campus_name_processed = None

                        if campus_name_processed != "REJECT":
                            # æ£€æŸ¥æ ¡åŒºåæ˜¯å¦å·²å­˜åœ¨
                            if campus_name_processed not in processed_campus_names:
                                processed_campus_names.add(campus_name_processed)
                                processed_poi_ids.add(poi_id)  # æ·»åŠ åˆ°å…¨å±€IDé›†åˆ
                                location = poi.get("location", {})
                                campus_data = {
                                    "id": poi.get("id"),
                                    "name": campus_name_processed,
                                    "address": poi.get("address"),
                                    "province": poi.get("province"),
                                    "city": poi.get("city"),
                                    "district": poi.get("district"),
                                    "location": {
                                        "type": "Point",
                                        "coordinates": [
                                            location.get("lng"),
                                            location.get("lat"),
                                        ],
                                    },
                                }
                                school_output["campuses"].append(campus_data)
                                print(
                                    f"    [âœ…] {poi.get('title')} -> {campus_name_processed} (ID: {poi.get('id')})"
                                )
                            else:
                                print(
                                    f"    [â­ï¸] {poi.get('title')} -> {campus_name_processed} (æœ¬æ ¡å†…åŒå)"
                                )
                        else:
                            rejected_pois.append(poi)
                            print(f"    [âŒ] {poi.get('title')}")
                else:
                    print(f"  - APIè¯·æ±‚å¤±è´¥æˆ–æ— æ•°æ®ï¼Œè·³è¿‡æ­¤å­¦æ ¡çš„åç»­è¯·æ±‚ã€‚")
                    break
                page_index += 1

            final_universities_data.append(school_output)

    except QuotaExceededError:
        print("\nå› APIæ¯æ—¥è°ƒç”¨é‡å·²è¾¾ä¸Šé™ï¼Œå¤„ç†ä¸­æ–­ã€‚")
        quota_exceeded = True

    # 5. å†™å…¥æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    print("\n--- å¤„ç†å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ ---")

    universities_with_campuses = []
    universities_without_campuses = []
    for school in final_universities_data:
        if school.get("campuses"):
            universities_with_campuses.append(school)
        else:
            universities_without_campuses.append(school)

    # å†™å…¥ä¸»JSONæ–‡ä»¶ (æœ‰æ ¡åŒº)
    with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(universities_with_campuses, f, ensure_ascii=False, indent=4)
    print(
        f"âœ… æœ‰æ ¡åŒºçš„å¤§å­¦æ•°æ®å·²å†™å…¥: {OUTPUT_JSON_FILE} ({len(universities_with_campuses)} æ¡)"
    )

    # å†™å…¥æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦CSVæ–‡ä»¶
    if universities_without_campuses:
        # ç¡®ä¿campusesé”®å­˜åœ¨ï¼Œå³ä½¿ä¸ºç©º
        for school in universities_without_campuses:
            if "campuses" not in school:
                school["campuses"] = []
        header = universities_without_campuses[0].keys()
        with open(NO_CAMPUSES_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            writer.writerows(universities_without_campuses)
        print(
            f"âœ… æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦åˆ—è¡¨å·²å†™å…¥: {NO_CAMPUSES_CSV_FILE} ({len(universities_without_campuses)} æ¡)"
        )

    # å†™å…¥è¢«æ‹’ç»çš„POI
    if rejected_pois:
        header = sorted(list(set(key for poi in rejected_pois for key in poi.keys())))
        with open(REJECTED_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            writer.writerows(rejected_pois)
        print(
            f"âœ… è¢«æ‹’ç»çš„POIåˆ—è¡¨å·²å†™å…¥: {REJECTED_CSV_FILE} ({len(rejected_pois)} æ¡)"
        )

    # å†™å…¥æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡
    if schools_without_details:
        # ç¡®ä¿IDä¸ºå­—ç¬¦ä¸²æ ¼å¼
        for school in schools_without_details:
            school["id"] = str(int(school["id"]))
        with open(NO_DETAILS_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=schools_without_details[0].keys())
            writer.writeheader()
            writer.writerows(schools_without_details)
        print(
            f"âœ… æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡åˆ—è¡¨å·²å†™å…¥: {NO_DETAILS_CSV_FILE} ({len(schools_without_details)} æ¡)"
        )

    # å¦‚æœæ˜¯å› ä¸ºé…é¢è€—å°½è€Œé€€å‡ºï¼Œåˆ™ä½¿ç”¨é”™è¯¯ç 
    if quota_exceeded:
        print("\nè„šæœ¬å› APIé…é¢è€—å°½è€Œç»ˆæ­¢ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚é€€å‡ºçŠ¶æ€ç : 1")
        sys.exit(1)


# --- è„šæœ¬å…¥å£ ---
if __name__ == "__main__":
    # --- è®¾ç½®æ—¥å¿—è®°å½• ---
    log_filename = f"{LOG_FILE_PREFIX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    sys.stdout = Logger(log_filename)

    print(f"æ—¥å¿—å°†è®°å½•åˆ°æ–‡ä»¶: {log_filename}")
    print("-" * 50)

    if not os.path.exists(EXCEL_FILE):
        print("-" * 50)
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{EXCEL_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å°†æ‚¨çš„Excelæ–‡ä»¶ä¸æ­¤è„šæœ¬æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œ")
        print("å¹¶ç¡®ä¿è„šæœ¬ä¸­çš„ 'EXCEL_FILE' å˜é‡å€¼æ­£ç¡®ã€‚")
        print("-" * 50)
    else:
        process_university_data(EXCEL_FILE)
