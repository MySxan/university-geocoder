import csv
import hashlib
import json
import math
import os
import re
import sys
import time
from datetime import datetime
from urllib.parse import urlencode

import pandas as pd
import requests
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- å…¨å±€é…ç½® ---
# ä»ç¯å¢ƒå˜é‡ä¸­è·å–Keyå’ŒSK
# è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå¹¶å¡«å…¥ä»¥ä¸‹å†…å®¹:
# TENCENT_MAP_KEY="ä½ çš„Key"
# TENCENT_MAP_SK="ä½ çš„Secret Key"
MY_KEY = os.getenv("TENCENT_MAP_KEY")
MY_SK = os.getenv("TENCENT_MAP_SK")


# APIç›¸å…³é…ç½®
API_HOST = "https://apis.map.qq.com"
API_PATH = "/ws/place/v1/suggestion"
PAGE_SIZE = 20
MAX_RETRIES = 10  # QPSè¶…é™æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 1  # é‡è¯•å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# è¾“å…¥æ–‡ä»¶é…ç½®
EXCEL_FILE = "univ_moe.xls"
SUPP_JSON_FILE = "univ_supp.json"

# è¾“å‡ºæ–‡ä»¶é…ç½®
OUTPUT_JSON_FILE = "universities.json"
REJECTED_CSV_FILE = "rejected_pois.csv"
NO_DETAILS_CSV_FILE = "universities_without_details.csv"
NO_CAMPUSES_CSV_FILE = "universities_with_no_campuses.csv"
LOG_FILE_PREFIX = "run_log"


class QuotaExceededError(Exception):
    """å½“APIæ¯æ—¥è°ƒç”¨é‡è¾¾åˆ°ä¸Šé™æ—¶æŠ›å‡ºçš„è‡ªå®šä¹‰å¼‚å¸¸ã€‚"""

    pass


class Logger(object):
    """
    å°†æ§åˆ¶å°è¾“å‡ºåŒæ—¶å†™å…¥æ–‡ä»¶ã€‚
    """

    def __init__(self, filename="default.log"):
        self.terminal = sys.stdout
        # ä½¿ç”¨ 'w' æ¨¡å¼ï¼Œæ¯æ¬¡è¿è¡Œåˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        # è¿™ä¸ª flush æ–¹æ³•æ˜¯ä¸º Python 3 å…¼å®¹æ€§æ‰€å¿…éœ€çš„ã€‚
        pass


def request_tencent_api(path: str, params: dict, sk: str):
    """
    ä¸€ä¸ªç”¨äºè¯·æ±‚è…¾è®¯åœ°å›¾Web APIçš„ä»£ç†å‡½æ•°ï¼Œå¯è‡ªåŠ¨å¤„ç†ç­¾åå’ŒQPSè¶…é™é‡è¯•ã€‚
    """
    for attempt in range(MAX_RETRIES):
        # ç­¾åè®¡ç®—
        sorted_params = sorted(params.items())
        qs_for_sig = "&".join([f"{k}={v}" for k, v in sorted_params])
        string_to_sign = f"{path}?{qs_for_sig}{sk}"
        sig = hashlib.md5(string_to_sign.encode("utf-8")).hexdigest()

        # æ„é€ æœ€ç»ˆè¯·æ±‚URL
        encoded_params = urlencode(params)
        final_url = f"{API_HOST}{path}?{encoded_params}&sig={sig}"

        try:
            response = requests.get(final_url, timeout=15)
            response.raise_for_status()
            response_data = response.json()

            # æ£€æŸ¥APIè¿”å›çš„çŠ¶æ€ç 
            status = response_data.get("status")
            if status == 0:
                return response_data  # è¯·æ±‚æˆåŠŸ
            elif status == 120:
                print(
                    f"  - è§¦å‘QPSé™åˆ¶ (status 120)ï¼Œå°†åœ¨ {RETRY_DELAY} ç§’åé‡è¯•... (ç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡)"
                )
                time.sleep(RETRY_DELAY)
                continue  # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
            else:
                message = response_data.get("message")
                print(f"  - APIè¿”å›é”™è¯¯: status={status}, message={message}")
                if status == 121:
                    raise QuotaExceededError(message)
                return None

        except requests.exceptions.Timeout:
            print(f"  - è¯·æ±‚è¶…æ—¶: {final_url}")
        except requests.exceptions.HTTPError as e:
            print(f"  - HTTPé”™è¯¯: {e.response.status_code} for url: {final_url}")
        except requests.exceptions.RequestException as e:
            print(f"  - è¯·æ±‚å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

        # å¦‚æœä¸æ˜¯å› ä¸ºQPSé™åˆ¶å¯¼è‡´çš„å¤±è´¥ï¼Œåˆ™ç¨ä½œç­‰å¾…åé‡è¯•
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY)

    print(f"  - é‡è¯• {MAX_RETRIES} æ¬¡åä»ç„¶å¤±è´¥ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚")
    return None


def post_process_name(name: str) -> str | None:
    """å¯¹æ ¡åŒºåç§°è¿›è¡Œåå¤„ç†"""
    if not name:
        return None

    # 1. trim
    processed_name = name.strip()
    # 2. å»é™¤â€œ-â€å’Œâ€œ&â€
    processed_name = processed_name.replace("-", "").replace("&", "")
    # 3. å»é™¤ç»“å°¾çš„ç‰¹å®šæ–¹ä½å’ŒæœŸæ•°å­—æ ·
    suffixes_to_remove = [
        "è¥¿åŒº",
        "ä¸œåŒº",
        "åŒ—åŒº",
        "å—åŒº",
        "ä¸­åŒº",
        "ä¸€æœŸ",
        "äºŒæœŸ",
        "ä¸‰æœŸ",
        "å››æœŸ",
        "äº”æœŸ",
        "å…­æœŸ",
        "ä¸ƒæœŸ",
        "å…«æœŸ",
        "ä¹æœŸ",
        "åæœŸ",
    ]
    for suffix in suffixes_to_remove:
        if processed_name.endswith(suffix):
            processed_name = processed_name[: -len(suffix)]

    # 4. å°†â€œXXä¸»æ ¡åŒºYYâ€æ›¿æ¢ä¸ºâ€œXXæ ¡åŒºYYâ€
    processed_name = re.sub(r"(.+)ä¸»æ ¡åŒº", r"\1æ ¡åŒº", processed_name).strip()

    return processed_name if processed_name else None


def is_valid_campus_name(name: str | None) -> bool:
    """æ£€æŸ¥åç§°æ˜¯å¦ç¬¦åˆæ ¡åŒºåå®šä¹‰"""
    if not name:
        return True
    # éé™„å±
    if "é™„å±" in name or "åŒ»é™¢" in name:
        return False
    # ä»¥ç‰¹å®šè¯ç»“å°¾
    valid_endings = ["æ ¡åŒº", "å›­åŒº", "é™¢åŒº", "æ ¡å›­", "å­¦æ ¡", "åˆ†æ ¡", "é™¢"]
    for ending in valid_endings:
        if name.endswith(ending):
            return True
    return False


def is_location_substring(text: str | None, poi: dict) -> bool:
    """æ£€æŸ¥æ–‡æœ¬æ˜¯å¦æ˜¯çœã€å¸‚ã€åŒºä¹‹ä¸€çš„å­å­—ç¬¦ä¸²ã€‚"""
    if not text:
        return False
    poi_province = poi.get("province", "")
    poi_city = poi.get("city", "")
    poi_district = poi.get("district", "")

    return (
        (poi_province and text in poi_province)
        or (poi_city and text in poi_city)
        or (poi_district and text in poi_district)
    )


def extract_bracketed_content(text: str) -> str:
    """
    æå–æ–‡æœ¬ä¸­æ‹¬å·å†…çš„å†…å®¹ï¼Œå¹¶è¿”å›å»é™¤æ‹¬å·åçš„å†…å®¹ã€‚
    """
    text = text.strip()
    if not text:
        return ""
    if text.startswith("(") and text.endswith(")"):
        text = text[1:-1]
    return text.strip()


def parse_campus_name(poi: dict, school_name: str):
    """
    æ ¹æ®æ–°çš„å¤æ‚è§„åˆ™è§£æPOIæ ‡é¢˜ä»¥æå–æ ¡åŒºåç§°ã€‚
    """
    poi_title = poi.get("title", "")

    # 1. çµæ´»çš„å‰ç¼€åŒ¹é…
    pattern_str = (
        re.escape(school_name).replace(r"ï¼ˆ", r"[\(ï¼ˆ]").replace(r"ï¼‰", r"[\)ï¼‰]")
    )
    pattern = re.compile(f"^{pattern_str}")
    match = pattern.match(poi_title)

    if not match:
        return "REJECT"

    if match.end() == len(poi_title):
        return None

    remaining_title = poi_title[match.end() :].strip()

    if (not remaining_title) or (remaining_title in ["ä¸»æ ¡åŒº", "æ ¡æœ¬éƒ¨"]):
        return None

    # 2. å°†å‰©ä½™éƒ¨åˆ†åˆ†å‰²ä¸ºå¸¦æ‹¬å·å’Œä¸å¸¦æ‹¬å·çš„ç‰‡æ®µ
    parts = [p for p in re.split(r"(\([^)]+\))", remaining_title) if p]

    # 3. ä»åå‘å‰éå†ç‰‡æ®µï¼Œå¯»æ‰¾ç¬¬ä¸€ä¸ªæœ‰æ•ˆçš„â€œé”šç‚¹â€
    for i in range(len(parts) - 1, -1, -1):
        current_part = parts[i]
        # æå–ç‰‡æ®µå†…å®¹ï¼ˆæ— è®ºæ˜¯å¦åœ¨æ‹¬å·å†…ï¼‰
        content = extract_bracketed_content(current_part)

        post_processed_content = post_process_name(content)

        # æ£€æŸ¥æ­¤ç‰‡æ®µæ˜¯å¦ä¸ºæœ‰æ•ˆé”šç‚¹
        is_campus_name = is_valid_campus_name(post_processed_content)
        is_loc_substr = is_location_substring(post_processed_content, poi)
        is_anchor = is_campus_name or is_loc_substr

        if is_anchor:
            # 4. å¦‚æœæ‰¾åˆ°é”šç‚¹ï¼Œæ‹¼æ¥ä»å¼€å¤´åˆ°æ­¤é”šç‚¹çš„æ‰€æœ‰éƒ¨åˆ†
            final_name_parts = []
            for j in range(i + 1):
                final_name_parts.append(extract_bracketed_content(parts[j]))

            final_name = "".join(final_name_parts)

            # å¦‚æœæœ€ç»ˆæ‹¼æ¥çš„åç§°æœ¬èº«ä¸ç¬¦åˆè§„åˆ™ï¼ˆé€šå¸¸æ˜¯å› ä¸ºé è¡Œæ”¿åŒºåˆ’åŒ¹é…ä¸Šçš„ï¼‰
            # åˆ™ä¸ºå…¶è¡¥ä¸Šâ€œæ ¡åŒºâ€åç¼€
            if (
                not is_valid_campus_name(post_process_name(final_name))
                and is_loc_substr
            ):
                final_name += "æ ¡åŒº"

            return post_process_name(final_name)

    return "REJECT"


def process_university_data(excel_path: str):
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œä»è¯»å–æ–‡ä»¶ã€åˆå¹¶æ•°æ®ã€APIè¯·æ±‚åˆ°è¾“å‡ºç»“æœçš„å®Œæ•´æµç¨‹ã€‚
    """
    if not MY_KEY or not MY_SK:
        print("é”™è¯¯ï¼šæ— æ³•ä» .env æ–‡ä»¶ä¸­åŠ è½½ TENCENT_MAP_KEY æˆ– TENCENT_MAP_SKã€‚")
        print("è¯·ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•ä¸‹å­˜åœ¨ .env æ–‡ä»¶ï¼Œå¹¶ä¸”å…¶ä¸­åŒ…å«æ­£ç¡®çš„Keyå’ŒSKã€‚")
        return

    # 1. è¯»å–Excelæ–‡ä»¶
    try:
        print(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {excel_path}")
        df_temp = pd.read_excel(excel_path, header=None)
        header_row_index = -1
        for i, row in df_temp.iterrows():
            if not isinstance(i, int):
                continue
            if "å­¦æ ¡åç§°" in str(row.values):
                header_row_index = i
                break
        if header_row_index == -1:
            print("é”™è¯¯ï¼šåœ¨Excelæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åŒ…å«â€œå­¦æ ¡åç§°â€çš„è¡¨å¤´è¡Œã€‚")
            return
        print(f"æ£€æµ‹åˆ°è¡¨å¤´åœ¨ç¬¬ {header_row_index + 1} è¡Œã€‚")
        df = pd.read_excel(excel_path, header=header_row_index)
    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 2. åŠ è½½ç”¨äºåˆå¹¶çš„é™„åŠ JSONæ•°æ®
    supp_data_map = {}
    try:
        print(f"æ­£åœ¨åŠ è½½é™„åŠ æ•°æ®æ–‡ä»¶: {SUPP_JSON_FILE}")
        with open(SUPP_JSON_FILE, "r", encoding="utf-8") as f:
            supp_list = json.load(f)
            supp_data_map = {item["name"]: item for item in supp_list}
        print(f"æˆåŠŸåŠ è½½ {len(supp_data_map)} æ¡é™„åŠ æ•°æ®ç”¨äºåˆå¹¶ã€‚")
    except Exception as e:
        print(f"åŠ è½½ '{SUPP_JSON_FILE}' æ—¶å‡ºé”™: {e}ï¼Œå°†è·³è¿‡æ•°æ®åˆå¹¶æ­¥éª¤ã€‚")

    # 3. æ¸…ç†å’Œé‡å‘½ååˆ—
    column_mapping = {
        "å­¦æ ¡æ ‡è¯†ç ": "id",
        "å­¦æ ¡åç§°": "name",
        "ä¸»ç®¡éƒ¨é—¨": "affiliation",
        "åŠå­¦å±‚æ¬¡": "type",
    }
    # åªä¿ç•™éœ€è¦çš„åˆ—
    df = df[list(column_mapping.keys())].rename(columns=column_mapping)
    df.dropna(subset=["name"], inplace=True)

    # æ¸…ç†IDåˆ—ï¼Œç¡®ä¿ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„æ•´æ•°
    df.dropna(subset=["id"], inplace=True)
    df["id"] = df["id"].astype(float).astype(int).astype(str)

    # æ¸…ç†å­¦æ ¡åç§°
    def clean_name(name):
        if isinstance(name, str):
            if name.startswith("æ°‘åŠ"):
                name = name[2:]
            # å»æ‰â€œå¸‚â€ï¼Œä½†ä¸å»æ‰â€œåŸå¸‚â€æˆ–â€œéƒ½å¸‚â€
            name = re.sub(r"(?<![åŸéƒ½])å¸‚", "", name)
        return name

    df["name"] = df["name"].apply(clean_name)

    # åˆå§‹åŒ–ç»“æœå’ŒæŠ¥å‘Šåˆ—è¡¨
    universities_list = df.to_dict("records")
    rejected_pois = []
    final_universities_data_map = {}  # ä½¿ç”¨mapä»¥æ–¹ä¾¿æŸ¥æ‰¾
    schools_without_details = []
    processed_pois_map = {}  # å­˜å‚¨æ›´è¯¦ç»†çš„åŒ¹é…ä¿¡æ¯
    quota_exceeded = False  # ç”¨äºæ ‡è®°é…é¢æ˜¯å¦è€—å°½

    print(f"æˆåŠŸè¯»å–å¹¶æ¸…ç†äº† {len(universities_list)} æ‰€å­¦æ ¡ã€‚å¼€å§‹å¤„ç†...")

    try:  # åŒ…è£¹ä¸»å¾ªç¯ä»¥ä¾¿æ•è·é…é¢å¼‚å¸¸
        # 4. éå†æ¯æ‰€å­¦æ ¡
        for index, school_from_xls in enumerate(universities_list):
            school_name = school_from_xls.get("name")
            if not school_name:
                print(
                    f"\n[{index + 1}/{len(universities_list)}] å­¦æ ¡åç§°ä¸ºç©ºï¼Œè·³è¿‡æ­¤æ¡è®°å½•ã€‚"
                )
                continue
            print(f"\n[{index + 1}/{len(universities_list)}] æ­£åœ¨æŸ¥è¯¢: {school_name}")

            school_output = {
                "id": school_from_xls.get("id"),
                "name": school_name,
                "affiliation": school_from_xls.get("affiliation"),
                "type": school_from_xls.get("type"),
            }

            # åˆå¹¶é™„åŠ æ•°æ®
            fields_to_merge = [
                "majorCategory",
                "natureOfRunning",
                "is985",
                "is211",
                "isDoubleFirstClass",
            ]
            if school_name in supp_data_map:
                details = supp_data_map[school_name]
                for field in fields_to_merge:
                    if field in details:
                        school_output[field] = details[field]
            else:
                print(f"  - åœ¨ {SUPP_JSON_FILE} ä¸­æœªæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
                for field in fields_to_merge:
                    school_output[field] = None
                schools_without_details.append(school_from_xls)

            school_output["campuses"] = []
            final_universities_data_map[school_name] = school_output  # å­˜å…¥map

            page_index = 1
            total_pages = 1

            # å¤„ç†åˆ†é¡µAPIè¯·æ±‚
            while page_index <= total_pages:
                print(f"  - æ­£åœ¨è¯·æ±‚ç¬¬ {page_index}/{total_pages} é¡µ...")
                params = {
                    "keyword": school_name,
                    "key": MY_KEY,
                    "filter": "category=å¤§å­¦",
                    "get_ad": 1,
                    "page_size": PAGE_SIZE,
                    "page_index": page_index,
                    "added_fields": "category_code",
                }
                response_data = request_tencent_api(API_PATH, params, MY_SK)
                time.sleep(0.2)

                if response_data:
                    if page_index == 1:
                        count = response_data.get("count", 0)
                        total_pages = math.ceil(count / PAGE_SIZE)

                    for poi in response_data.get("data", []):
                        poi_id = poi.get("id")
                        poi_title = poi.get("title", "")
                        if not poi_id or not poi_title:
                            continue

                        campus_name_processed = parse_campus_name(poi, school_name)

                        if campus_name_processed == "REJECT":
                            rejected_pois.append(poi)
                            print(f"    [âŒ] {poi.get('title')}")
                            continue

                        # --- æœ€é•¿å‰ç¼€å æ¯”åŒ¹é…æ ¸å¿ƒé€»è¾‘ ---
                        current_prefix_ratio = len(school_name) / len(poi_title)
                        previous_match = processed_pois_map.get(poi_id)

                        if (
                            not previous_match
                            or current_prefix_ratio > previous_match["prefix_ratio"]
                        ):
                            current_school_data = final_universities_data_map[
                                school_name
                            ]
                            existing_campus_names = {
                                c["name"] for c in current_school_data["campuses"]
                            }

                            if previous_match:
                                prev_school_name = previous_match["school_name"]
                                prev_school_output = final_universities_data_map.get(
                                    prev_school_name
                                )
                                if prev_school_output:
                                    prev_school_output["campuses"] = [
                                        c
                                        for c in prev_school_output["campuses"]
                                        if c.get("id") != poi_id
                                    ]
                                    print(
                                        f"    [ğŸ”„] {poi_title}: '{prev_school_name}' (å æ¯” {previous_match['prefix_ratio']:.2f}) â†’ '{school_name}' (å æ¯” {current_prefix_ratio:.2f})"
                                    )

                            if campus_name_processed in existing_campus_names:
                                print(
                                    f"    [â­ï¸] {poi_title} â†’ {campus_name_processed} (æ ¡å†…åŒå)"
                                )
                                continue

                            location = poi.get("location", {})
                            campus_data = {
                                "id": poi_id,
                                "name": campus_name_processed,
                                "address": poi.get("address"),
                                "province": poi.get("province"),
                                "city": poi.get("city"),
                                "district": poi.get("district"),
                                "location": {
                                    "type": "Point",
                                    "coordinates": [
                                        location.get("lng"),
                                        location.get("lat"),
                                    ],
                                },
                            }
                            current_school_data["campuses"].append(campus_data)

                            processed_pois_map[poi_id] = {
                                "school_name": school_name,
                                "prefix_ratio": current_prefix_ratio,
                            }
                            print(
                                f"    [âœ…] {poi_title} â†’ {campus_name_processed} (ID: {poi_id})"
                            )

                        else:
                            print(
                                f"    [ğŸŒ] {poi_title} (ID: {poi_id}, å·²åˆ†é…ç»™æ›´ä¼˜åŒ¹é… '{previous_match['school_name']}' å æ¯” {previous_match['prefix_ratio']:.2f})"
                            )
                else:
                    print(f"  - APIè¯·æ±‚å¤±è´¥æˆ–æ— æ•°æ®ï¼Œè·³è¿‡æ­¤å­¦æ ¡çš„åç»­è¯·æ±‚ã€‚")
                    break
                page_index += 1

    except QuotaExceededError:
        print("\nå› APIæ¯æ—¥è°ƒç”¨é‡å·²è¾¾ä¸Šé™ï¼Œå¤„ç†ä¸­æ–­ã€‚")
        quota_exceeded = True

    # 5. å†™å…¥æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    print("\n--- å¤„ç†å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ ---")

    final_universities_data = list(
        final_universities_data_map.values()
    )  # ä»mapè½¬å›list
    universities_with_campuses = []
    universities_without_campuses = []
    for school in final_universities_data:
        if school.get("campuses"):
            universities_with_campuses.append(school)
        else:
            universities_without_campuses.append(school)

    # å†™å…¥ä¸»JSONæ–‡ä»¶ (æœ‰æ ¡åŒº)
    with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(universities_with_campuses, f, ensure_ascii=False, indent=4)
    print(
        f"âœ… æœ‰æ ¡åŒºçš„å¤§å­¦æ•°æ®å·²å†™å…¥: {OUTPUT_JSON_FILE} ({len(universities_with_campuses)} æ¡)"
    )

    # å†™å…¥æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦CSVæ–‡ä»¶
    if universities_without_campuses:
        # ç¡®ä¿campusesé”®å­˜åœ¨ï¼Œå³ä½¿ä¸ºç©º
        for school in universities_without_campuses:
            if "campuses" not in school:
                school["campuses"] = []
        header = universities_without_campuses[0].keys()
        with open(NO_CAMPUSES_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            writer.writerows(universities_without_campuses)
        print(
            f"âœ… æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦åˆ—è¡¨å·²å†™å…¥: {NO_CAMPUSES_CSV_FILE} ({len(universities_without_campuses)} æ¡)"
        )

    # å†™å…¥è¢«æ‹’ç»çš„POI
    if rejected_pois:
        header = sorted(list(set(key for poi in rejected_pois for key in poi.keys())))
        with open(REJECTED_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            writer.writerows(rejected_pois)
        print(
            f"âœ… è¢«æ‹’ç»çš„POIåˆ—è¡¨å·²å†™å…¥: {REJECTED_CSV_FILE} ({len(rejected_pois)} æ¡)"
        )

    # å†™å…¥æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡
    if schools_without_details:
        # ç¡®ä¿IDä¸ºå­—ç¬¦ä¸²æ ¼å¼
        for school in schools_without_details:
            school["id"] = str(int(school["id"]))
        with open(NO_DETAILS_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=schools_without_details[0].keys())
            writer.writeheader()
            writer.writerows(schools_without_details)
        print(
            f"âœ… æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡åˆ—è¡¨å·²å†™å…¥: {NO_DETAILS_CSV_FILE} ({len(schools_without_details)} æ¡)"
        )

    # å¦‚æœæ˜¯å› ä¸ºé…é¢è€—å°½è€Œé€€å‡ºï¼Œåˆ™ä½¿ç”¨é”™è¯¯ç 
    if quota_exceeded:
        print("\nè„šæœ¬å› APIé…é¢è€—å°½è€Œç»ˆæ­¢ï¼Œå·²ä¿å­˜å½“å‰è¿›åº¦ã€‚é€€å‡ºçŠ¶æ€ç : 1")
        sys.exit(1)


# --- è„šæœ¬å…¥å£ ---
if __name__ == "__main__":
    # --- è®¾ç½®æ—¥å¿—è®°å½• ---
    log_filename = f"{LOG_FILE_PREFIX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    sys.stdout = Logger(log_filename)

    print(f"æ—¥å¿—å°†è®°å½•åˆ°æ–‡ä»¶: {log_filename}")
    print("-" * 50)

    if not os.path.exists(EXCEL_FILE):
        print("-" * 50)
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{EXCEL_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å°†æ‚¨çš„Excelæ–‡ä»¶ä¸æ­¤è„šæœ¬æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œ")
        print("å¹¶ç¡®ä¿è„šæœ¬ä¸­çš„ 'EXCEL_FILE' å˜é‡å€¼æ­£ç¡®ã€‚")
        print("-" * 50)
    else:
        process_university_data(EXCEL_FILE)
