import csv
import hashlib
import json
import math
import os
import re
import sys
import time
from datetime import datetime
from urllib.parse import urlencode

import pandas as pd
import requests
from dotenv import load_dotenv

# åŠ è½½ .env æ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡
load_dotenv()

# --- å…¨å±€é…ç½® ---
# ä»ç¯å¢ƒå˜é‡ä¸­è·å–Keyå’ŒSK
# è¯·åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º .env æ–‡ä»¶ï¼Œå¹¶å¡«å…¥ä»¥ä¸‹å†…å®¹:
# TENCENT_MAP_KEY="ä½ çš„Key"
# TENCENT_MAP_SK="ä½ çš„Secret Key"
MY_KEY = os.getenv("TENCENT_MAP_KEY")
MY_SK = os.getenv("TENCENT_MAP_SK")


# APIç›¸å…³é…ç½®
API_HOST = "https://apis.map.qq.com"
API_PATH = "/ws/place/v1/suggestion"
PAGE_SIZE = 20
MAX_RETRIES = 10  # QPSè¶…é™æ—¶çš„æœ€å¤§é‡è¯•æ¬¡æ•°
RETRY_DELAY = 2  # é‡è¯•å‰çš„ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

# è¾“å…¥/è¾“å‡ºæ–‡ä»¶é…ç½®
EXCEL_FILE = "univ_moe.xls"
SUPP_JSON_FILE = "univ_supp.json"
OUTPUT_JSON_FILE = "universities.json"
REJECTED_CSV_FILE = "rejected_pois.csv"
NO_POI_CSV_FILE = "universities_with_no_poi.csv"
NO_DETAILS_CSV_FILE = "universities_without_details.csv"
NO_CAMPUSES_JSON_FILE = "universities_with_no_campuses.json"
LOG_FILE_PREFIX = "run_log"


# --- æ—¥å¿—è®°å½•ç±» ---
class Logger(object):
    """
    å°†æ§åˆ¶å°è¾“å‡ºåŒæ—¶å†™å…¥æ–‡ä»¶ã€‚
    """

    def __init__(self, filename="default.log"):
        self.terminal = sys.stdout
        # ä½¿ç”¨ 'w' æ¨¡å¼ï¼Œæ¯æ¬¡è¿è¡Œåˆ›å»ºæ–°çš„æ—¥å¿—æ–‡ä»¶
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        # è¿™ä¸ª flush æ–¹æ³•æ˜¯ä¸º Python 3 å…¼å®¹æ€§æ‰€å¿…éœ€çš„ã€‚
        pass


def request_tencent_api(path: str, params: dict, sk: str):
    """
    ä¸€ä¸ªç”¨äºè¯·æ±‚è…¾è®¯åœ°å›¾Web APIçš„ä»£ç†å‡½æ•°ï¼Œå¯è‡ªåŠ¨å¤„ç†ç­¾åå’ŒQPSè¶…é™é‡è¯•ã€‚
    """
    for attempt in range(MAX_RETRIES):
        # ç­¾åè®¡ç®—
        sorted_params = sorted(params.items())
        qs_for_sig = "&".join([f"{k}={v}" for k, v in sorted_params])
        string_to_sign = f"{path}?{qs_for_sig}{sk}"
        sig = hashlib.md5(string_to_sign.encode("utf-8")).hexdigest()

        # æ„é€ æœ€ç»ˆè¯·æ±‚URL
        encoded_params = urlencode(params)
        final_url = f"{API_HOST}{path}?{encoded_params}&sig={sig}"

        try:
            response = requests.get(final_url, timeout=15)
            response.raise_for_status()
            response_data = response.json()

            # æ£€æŸ¥APIè¿”å›çš„çŠ¶æ€ç 
            status = response_data.get("status")
            if status == 0:
                return response_data  # è¯·æ±‚æˆåŠŸ
            elif status == 120:
                print(
                    f"  - è§¦å‘QPSé™åˆ¶ (status 120)ï¼Œå°†åœ¨ {RETRY_DELAY} ç§’åé‡è¯•... (ç¬¬ {attempt + 1}/{MAX_RETRIES} æ¬¡)"
                )
                time.sleep(RETRY_DELAY)
                continue  # è¿›å…¥ä¸‹ä¸€æ¬¡é‡è¯•
            else:
                print(
                    f"  - APIè¿”å›é”™è¯¯: status={status}, message={response_data.get('message')}"
                )
                return None

        except requests.exceptions.Timeout:
            print(f"  - è¯·æ±‚è¶…æ—¶: {final_url}")
        except requests.exceptions.HTTPError as e:
            print(f"  - HTTPé”™è¯¯: {e.response.status_code} for url: {final_url}")
        except requests.exceptions.RequestException as e:
            print(f"  - è¯·æ±‚å‘ç”Ÿä¸¥é‡é”™è¯¯: {e}")

        # å¦‚æœä¸æ˜¯å› ä¸ºQPSé™åˆ¶å¯¼è‡´çš„å¤±è´¥ï¼Œåˆ™ç¨ä½œç­‰å¾…åé‡è¯•
        if attempt < MAX_RETRIES - 1:
            time.sleep(RETRY_DELAY)

    print(f"  - é‡è¯• {MAX_RETRIES} æ¬¡åä»ç„¶å¤±è´¥ï¼Œæ”¾å¼ƒè¯·æ±‚ã€‚")
    return None


def post_process_name(name: str) -> str:
    """å¯¹æ ¡åŒºåç§°è¿›è¡Œåå¤„ç†"""
    if not name:
        return name

    # 1. trim
    processed_name = name.strip()
    # 2. å»é™¤â€œ-â€å’Œâ€œ&â€
    processed_name = processed_name.replace("-", "").replace("&", "")
    # 3. å»é™¤ç»“å°¾çš„ç‰¹å®šæ–¹ä½å’ŒæœŸæ•°å­—æ ·
    suffixes_to_remove = [
        "è¥¿åŒº",
        "ä¸œåŒº",
        "åŒ—åŒº",
        "å—åŒº",
        "ä¸­åŒº",
        "ä¸€æœŸ",
        "äºŒæœŸ",
        "ä¸‰æœŸ",
        "å››æœŸ",
        "äº”æœŸ",
        "å…­æœŸ",
        "ä¸ƒæœŸ",
        "å…«æœŸ",
        "ä¹æœŸ",
        "åæœŸ",
    ]
    for suffix in suffixes_to_remove:
        if processed_name.endswith(suffix):
            processed_name = processed_name[: -len(suffix)]
            break  # åªç§»é™¤ä¸€ä¸ª
    # 4. å°†â€œXXä¸»æ ¡åŒºYYâ€æ›¿æ¢ä¸ºâ€œXXæ ¡åŒºYYâ€
    processed_name = re.sub(r"(.+)ä¸»æ ¡åŒº", r"\1æ ¡åŒº", processed_name)

    return processed_name.strip()


def is_valid_campus_name(name: str) -> bool:
    """æ£€æŸ¥åç§°æ˜¯å¦ç¬¦åˆæ ¡åŒºåå®šä¹‰"""
    if not name:
        return False
    # éé™„å±
    if "é™„å±" in name or "åŒ»é™¢" in name:
        return False
    # ä»¥ç‰¹å®šè¯ç»“å°¾
    valid_endings = ["æ ¡åŒº", "å›­åŒº", "é™¢åŒº", "æ ¡å›­", "å­¦æ ¡", "åˆ†æ ¡", "é™¢"]
    for ending in valid_endings:
        if name.endswith(ending):
            return True
    return False


def check_and_get_name(part: str, poi_address: str):
    """
    è¾…åŠ©å‡½æ•°ï¼šæ£€æŸ¥ä¸€ä¸ªéƒ¨åˆ†æ˜¯å¦æ˜¯æœ‰æ•ˆæ ¡åŒºåï¼Œæˆ–èƒ½å¦é€šè¿‡åœ°å€è¾…åŠ©æˆä¸ºæ ¡åŒºå
    """
    processed_part = post_process_name(part)
    if is_valid_campus_name(processed_part):
        return processed_part

    # å¦‚æœåç§°æ˜¯åœ°å€çš„å­ä¸²ï¼Œåˆ™åŠ ä¸Šâ€œæ ¡åŒºâ€
    if processed_part and poi_address and processed_part in poi_address:
        return f"{processed_part}æ ¡åŒº"

    return None


def parse_campus_name(poi: dict, school_name: str):
    """
    æ ¹æ®è§„åˆ™è§£æPOIæ ‡é¢˜ä»¥æå–æ ¡åŒºåç§°ã€‚
    è¿”å›åå¤„ç†è¿‡çš„æ ¡åŒºåï¼Œå¦‚æœæ— åŒ¹é…åˆ™è¿”å›Noneã€‚
    """
    poi_title = poi.get("title", "")
    poi_address = poi.get("address", "")

    # é€šè¿‡çµæ´»çš„æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…å­¦æ ¡åï¼Œå¿½ç•¥æ‹¬å·å·®å¼‚
    pattern_str = (
        re.escape(school_name).replace(r"ï¼ˆ", r"[\(ï¼ˆ]").replace(r"ï¼‰", r"[\)ï¼‰]")
    )
    pattern = re.compile(f"^{pattern_str}")
    match = pattern.match(poi_title)

    # å¦‚æœå®Œå…¨åŒ¹é…ï¼ˆåŒ…æ‹¬æ‹¬å·å·®å¼‚ï¼‰ï¼Œä¹Ÿè§†ä¸ºnullæ ¡åŒº
    if match and match.end() == len(poi_title):
        return None

    if not match:
        return "REJECT"

    remaining_title = poi_title[match.end() :].strip()
    if not remaining_title:
        return None

    if remaining_title in ["ä¸»æ ¡åŒº", "æ ¡æœ¬éƒ¨"]:
        return None

    # æ‹¬å·è§£æ B_0(B_1)...(B_n)
    parts_in_parentheses = re.findall(r"\(([^)]+)\)", remaining_title)
    # ä»åå¾€å‰æ£€æŸ¥æ‹¬å·å†…å®¹ (B_n -> B_1)
    for part in reversed(parts_in_parentheses):
        campus_name = check_and_get_name(part, poi_address)
        if campus_name is not None:
            return campus_name

    # æ£€æŸ¥ç¬¬ä¸€ä¸ªæ‹¬å·å‰çš„å†…å®¹
    b0_part = remaining_title.split("(", 1)[0].strip()
    if b0_part:
        campus_name = check_and_get_name(b0_part, poi_address)
        if campus_name is not None:
            return campus_name

    # å¦‚æœæ²¡æœ‰æ‹¬å·ï¼Œæ£€æŸ¥æ•´ä¸ªå‰©ä½™éƒ¨åˆ†
    if not parts_in_parentheses:
        campus_name = check_and_get_name(remaining_title, poi_address)
        if campus_name is not None:
            return campus_name

    return "REJECT"


def process_university_data(excel_path: str):
    """
    ä¸»å‡½æ•°ï¼Œæ‰§è¡Œä»è¯»å–æ–‡ä»¶ã€åˆå¹¶æ•°æ®ã€APIè¯·æ±‚åˆ°è¾“å‡ºç»“æœçš„å®Œæ•´æµç¨‹ã€‚
    """
    if not MY_KEY or not MY_SK:
        print("é”™è¯¯ï¼šæ— æ³•ä» .env æ–‡ä»¶ä¸­åŠ è½½ TENCENT_MAP_KEY æˆ– TENCENT_MAP_SKã€‚")
        print("è¯·ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•ä¸‹å­˜åœ¨ .env æ–‡ä»¶ï¼Œå¹¶ä¸”å…¶ä¸­åŒ…å«æ­£ç¡®çš„Keyå’ŒSKã€‚")
        return

    # 1. è¯»å–Excelæ–‡ä»¶ (é‡å†™é€»è¾‘ä»¥å¤„ç†å¤æ‚è¡¨å¤´)
    try:
        print(f"æ­£åœ¨è¯»å–Excelæ–‡ä»¶: {excel_path}")
        df_temp = pd.read_excel(excel_path, header=None)
        header_row_index = -1
        for i, row in df_temp.iterrows():
            if "å­¦æ ¡åç§°" in str(row.values):
                header_row_index = i
                break
        if header_row_index == -1:
            print("é”™è¯¯ï¼šåœ¨Excelæ–‡ä»¶ä¸­æ‰¾ä¸åˆ°åŒ…å«â€œå­¦æ ¡åç§°â€çš„è¡¨å¤´è¡Œã€‚")
            return
        print(f"æ£€æµ‹åˆ°è¡¨å¤´åœ¨ç¬¬ {header_row_index + 1} è¡Œã€‚")
        df = pd.read_excel(excel_path, header=header_row_index)
    except Exception as e:
        print(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
        return

    # 2. åŠ è½½ç”¨äºåˆå¹¶çš„é™„åŠ JSONæ•°æ®
    supp_data_map = {}
    try:
        print(f"æ­£åœ¨åŠ è½½é™„åŠ æ•°æ®æ–‡ä»¶: {SUPP_JSON_FILE}")
        with open(SUPP_JSON_FILE, "r", encoding="utf-8") as f:
            supp_list = json.load(f)
            supp_data_map = {item["name"]: item for item in supp_list}
        print(f"æˆåŠŸåŠ è½½ {len(supp_data_map)} æ¡é™„åŠ æ•°æ®ç”¨äºåˆå¹¶ã€‚")
    except Exception as e:
        print(f"åŠ è½½ '{SUPP_JSON_FILE}' æ—¶å‡ºé”™: {e}ï¼Œå°†è·³è¿‡æ•°æ®åˆå¹¶æ­¥éª¤ã€‚")

    # 3. æ¸…ç†å’Œé‡å‘½ååˆ—
    column_mapping = {
        "å­¦æ ¡æ ‡è¯†ç ": "id",
        "å­¦æ ¡åç§°": "name",
        "ä¸»ç®¡éƒ¨é—¨": "affiliation",
        "åŠå­¦å±‚æ¬¡": "type",
    }
    # åªä¿ç•™éœ€è¦çš„åˆ—
    df = df[list(column_mapping.keys())].rename(columns=column_mapping)
    df.dropna(subset=["name"], inplace=True)

    # æ¸…ç†IDåˆ—ï¼Œç¡®ä¿ä¸ºå­—ç¬¦ä¸²æ ¼å¼çš„æ•´æ•°
    df.dropna(subset=["id"], inplace=True)
    df["id"] = df["id"].astype(float).astype(int).astype(str)

    # æ¸…ç†å­¦æ ¡åç§°ï¼Œå»æ‰â€œæ°‘åŠâ€å‰ç¼€
    df["name"] = df["name"].apply(
        lambda name: (
            name[2:] if isinstance(name, str) and name.startswith("æ°‘åŠ") else name
        )
    )

    # åˆå§‹åŒ–ç»“æœå’ŒæŠ¥å‘Šåˆ—è¡¨
    universities_list = df.to_dict("records")
    rejected_pois = []
    final_universities_data = []
    schools_with_no_poi = []
    schools_without_details = []
    processed_poi_ids = set()

    print(f"æˆåŠŸè¯»å–å¹¶æ¸…ç†äº† {len(universities_list)} æ‰€å­¦æ ¡ã€‚å¼€å§‹å¤„ç†...")

    # 4. éå†æ¯æ‰€å­¦æ ¡
    for index, school_from_xls in enumerate(universities_list):
        school_name = school_from_xls.get("name")
        print(f"\n[{index + 1}/{len(universities_list)}] æ­£åœ¨æŸ¥è¯¢: {school_name}")

        school_output = {
            "id": school_from_xls.get("id"),
            "name": school_name,
            "affiliation": school_from_xls.get("affiliation"),
            "type": school_from_xls.get("type"),
        }

        # åˆå¹¶é™„åŠ æ•°æ®
        fields_to_merge = [
            "majorCategory",
            "type",
            "is985",
            "is211",
            "isDoubleFirstClass",
        ]
        if school_name in supp_data_map:
            details = supp_data_map[school_name]
            for field in fields_to_merge:
                if field in details:
                    school_output[field] = details[field]
        else:
            print(f"  - åœ¨ {SUPP_JSON_FILE} ä¸­æœªæ‰¾åˆ°åŒ¹é…é¡¹ã€‚")
            for field in fields_to_merge:
                school_output[field] = None
            schools_without_details.append(school_from_xls)

        school_output["campuses"] = []
        processed_campus_names = set()  # ç”¨äºæ ¡åŒºå»é‡
        page_index = 1
        total_pages = 1
        found_poi = False

        # å¤„ç†åˆ†é¡µAPIè¯·æ±‚
        while page_index <= total_pages:
            print(f"  - æ­£åœ¨è¯·æ±‚ç¬¬ {page_index}/{total_pages} é¡µ...")
            params = {
                "keyword": school_name,
                "key": MY_KEY,
                "filter": "category=å¤§å­¦",
                "get_ad": 1,
                "page_size": PAGE_SIZE,
                "page_index": page_index,
                "added_fields": "category_code",
            }
            response_data = request_tencent_api(API_PATH, params, MY_SK)
            time.sleep(0.2)

            if response_data:
                if page_index == 1:
                    count = response_data.get("count", 0)
                    total_pages = math.ceil(count / PAGE_SIZE)

                for poi in response_data.get("data", []):
                    poi_id = poi.get("id")

                    if poi_id and poi_id in processed_poi_ids:
                        print(
                            f"    [ğŸŒ] {poi.get('title')} (ID: {poi_id} å·²åœ¨å…¨å±€ä¿å­˜)"
                        )
                        continue

                    campus_name_processed = parse_campus_name(poi, school_name)

                    if campus_name_processed != "REJECT":
                        # æ£€æŸ¥æ ¡åŒºåæ˜¯å¦å·²å­˜åœ¨
                        if campus_name_processed not in processed_campus_names:
                            processed_campus_names.add(campus_name_processed)
                            processed_poi_ids.add(poi_id)  # æ·»åŠ åˆ°å…¨å±€IDé›†åˆ
                            location = poi.get("location", {})
                            campus_data = {
                                "id": poi.get("id"),
                                "name": campus_name_processed,
                                "address": poi.get("address"),
                                "province": poi.get("province"),
                                "city": poi.get("city"),
                                "district": poi.get("district"),
                                "location": {
                                    "type": "Point",
                                    "coordinates": [
                                        location.get("lng"),
                                        location.get("lat"),
                                    ],
                                },
                            }
                            school_output["campuses"].append(campus_data)
                            found_poi = True
                            print(
                                f"    [âœ…] {poi.get('title')} (ID: {poi.get('id')}) -> {campus_name_processed}"
                            )
                        else:
                            print(
                                f"    [â­ï¸] {poi.get('title')} -> {campus_name_processed} (æœ¬æ ¡å†…åŒå)"
                            )
                    else:
                        rejected_pois.append(poi)
                        print(f"    [âŒ] {poi.get('title')}")
            else:
                print(f"  - APIè¯·æ±‚å¤±è´¥æˆ–æ— æ•°æ®ï¼Œè·³è¿‡æ­¤å­¦æ ¡çš„åç»­è¯·æ±‚ã€‚")
                break
            page_index += 1

        if not found_poi:
            print("  - æœªæ‰¾åˆ°ç›¸å…³POIã€‚")
            schools_with_no_poi.append(school_from_xls)

        final_universities_data.append(school_output)

    # 5. å†™å…¥æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
    print("\n--- å¤„ç†å®Œæˆï¼Œæ­£åœ¨ç”ŸæˆæŠ¥å‘Šæ–‡ä»¶ ---")

    universities_with_campuses = []
    universities_without_campuses = []
    for school in final_universities_data:
        if school.get("campuses"):
            universities_with_campuses.append(school)
        else:
            universities_without_campuses.append(school)

    # å†™å…¥ä¸»JSONæ–‡ä»¶ (æœ‰æ ¡åŒº)
    with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(universities_with_campuses, f, ensure_ascii=False, indent=4)
    print(
        f"âœ… æœ‰æ ¡åŒºçš„å¤§å­¦æ•°æ®å·²å†™å…¥: {OUTPUT_JSON_FILE} ({len(universities_with_campuses)} æ¡)"
    )

    # å†™å…¥æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦JSONæ–‡ä»¶
    if universities_without_campuses:
        with open(NO_CAMPUSES_JSON_FILE, "w", encoding="utf-8") as f:
            json.dump(universities_without_campuses, f, ensure_ascii=False, indent=4)
        print(
            f"âœ… æ²¡æœ‰æ ¡åŒºçš„å¤§å­¦åˆ—è¡¨å·²å†™å…¥: {NO_CAMPUSES_JSON_FILE} ({len(universities_without_campuses)} æ¡)"
        )

    # å†™å…¥è¢«æ‹’ç»çš„POI
    if rejected_pois:
        header = sorted(list(set(key for poi in rejected_pois for key in poi.keys())))
        with open(REJECTED_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=header)
            writer.writeheader()
            writer.writerows(rejected_pois)
        print(
            f"âœ… è¢«æ‹’ç»çš„POIåˆ—è¡¨å·²å†™å…¥: {REJECTED_CSV_FILE} ({len(rejected_pois)} æ¡)"
        )

    # å†™å…¥æœªæ‰¾åˆ°POIçš„å­¦æ ¡
    if schools_with_no_poi:
        with open(NO_POI_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=schools_with_no_poi[0].keys())
            writer.writeheader()
            writer.writerows(schools_with_no_poi)
        print(
            f"âœ… APIæœªè¿”å›ä»»ä½•POIçš„å­¦æ ¡åˆ—è¡¨å·²å†™å…¥: {NO_POI_CSV_FILE} ({len(schools_with_no_poi)} æ¡)"
        )

    # å†™å…¥æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡
    if schools_without_details:
        with open(NO_DETAILS_CSV_FILE, "w", encoding="utf-8-sig", newline="") as f:
            writer = csv.DictWriter(f, fieldnames=schools_without_details[0].keys())
            writer.writeheader()
            writer.writerows(schools_without_details)
        print(
            f"âœ… æœªæ‰¾åˆ°é™„åŠ ä¿¡æ¯çš„å­¦æ ¡åˆ—è¡¨å·²å†™å…¥: {NO_DETAILS_CSV_FILE} ({len(schools_without_details)} æ¡)"
        )


# --- è„šæœ¬å…¥å£ ---
if __name__ == "__main__":
    # --- è®¾ç½®æ—¥å¿—è®°å½• ---
    log_filename = f"{LOG_FILE_PREFIX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    sys.stdout = Logger(log_filename)

    print(f"æ—¥å¿—å°†è®°å½•åˆ°æ–‡ä»¶: {log_filename}")
    print("-" * 50)

    if not os.path.exists(EXCEL_FILE):
        print("-" * 50)
        print(f"é”™è¯¯: è¾“å…¥æ–‡ä»¶ '{EXCEL_FILE}' ä¸å­˜åœ¨ã€‚")
        print("è¯·å°†æ‚¨çš„Excelæ–‡ä»¶ä¸æ­¤è„šæœ¬æ”¾åœ¨åŒä¸€ç›®å½•ä¸‹ï¼Œ")
        print("å¹¶ç¡®ä¿è„šæœ¬ä¸­çš„ 'EXCEL_FILE' å˜é‡å€¼æ­£ç¡®ã€‚")
        print("-" * 50)
    else:
        process_university_data(EXCEL_FILE)
