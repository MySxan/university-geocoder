import os
import sys
import json
import csv
import time
from datetime import datetime
from pathlib import Path

import requests
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# ===== é…ç½® =====
RANKINGS_CSV = "merged_rankings.csv"
CACHE_FILE = "google_places_cache.json"
OUTPUT_JSON_FILE = "universities_global.json"
CHECKPOINT_FILE = "processing_checkpoint.json"  # æ–­ç‚¹ç»­ä¼ æ–‡ä»¶
LOG_FILE_PREFIX = "dedupe_log"

GOOGLE_MAPS_KEY = os.getenv("GOOGLE_MAPS_KEY")

# Google Places API (New) å‚æ•°
PLACES_API_NEW = "https://places.googleapis.com/v1"
REQUEST_DELAY = 0.05  # APIè¯·æ±‚é—´éš”ï¼ˆç§’ï¼‰
MAX_RETRIES = 3


# ===== æ£€æŸ¥ç‚¹ç®¡ç† =====
class CheckpointManager:
    """ç®¡ç†å¤„ç†è¿›åº¦çš„æ£€æŸ¥ç‚¹"""
    
    def __init__(self, checkpoint_file: str):
        self.checkpoint_file = checkpoint_file
        self.checkpoint = self._load_checkpoint()
    
    def _load_checkpoint(self) -> dict:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        if os.path.exists(self.checkpoint_file):
            try:
                with open(self.checkpoint_file, "r", encoding="utf-8") as f:
                    checkpoint = json.load(f)
                print(f"âœ… åŠ è½½æ£€æŸ¥ç‚¹: {self.checkpoint_file}")
                print(f"   - å·²å¤„ç†: {checkpoint.get('processed_count', 0)} æ¡")
                print(f"   - æˆåŠŸ: {checkpoint.get('success_count', 0)} æ¡")
                print(f"   - å·²æŸ¥è¯¢çš„å¤§å­¦: {len(checkpoint.get('processed_names', []))} ä¸ª")
                return checkpoint
            except Exception as e:
                print(f"âš ï¸ åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return self._create_empty_checkpoint()
        return self._create_empty_checkpoint()
    
    @staticmethod
    def _create_empty_checkpoint() -> dict:
        """åˆ›å»ºç©ºæ£€æŸ¥ç‚¹"""
        return {
            "processed_count": 0,      # å·²å¤„ç†çš„æ€»æ•°
            "success_count": 0,        # æˆåŠŸæŸ¥è¯¢çš„æ•°é‡
            "failed_count": 0,         # å¤±è´¥çš„æ•°é‡
            "processed_names": [],     # å·²å¤„ç†çš„å¤§å­¦åç§°åˆ—è¡¨
            "failed_names": [],        # å¤±è´¥çš„å¤§å­¦åç§°åˆ—è¡¨
            "created_at": datetime.now().isoformat(),
            "last_updated": datetime.now().isoformat(),
        }
    
    def save_checkpoint(self, processed_count: int, success_count: int, 
                       failed_count: int, processed_names: list, failed_names: list):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        self.checkpoint = {
            "processed_count": processed_count,
            "success_count": success_count,
            "failed_count": failed_count,
            "processed_names": processed_names,
            "failed_names": failed_names,
            "last_updated": datetime.now().isoformat(),
        }
        
        with open(self.checkpoint_file, "w", encoding="utf-8") as f:
            json.dump(self.checkpoint, f, ensure_ascii=False, indent=2)
    
    def get_processed_names(self) -> set:
        """è·å–å·²å¤„ç†çš„å¤§å­¦åç§°é›†åˆ"""
        return set(self.checkpoint.get("processed_names", []))
    
    def get_failed_names(self) -> set:
        """è·å–å¤±è´¥çš„å¤§å­¦åç§°é›†åˆ"""
        return set(self.checkpoint.get("failed_names", []))
    
    def is_completed(self, total_count: int) -> bool:
        """æ£€æŸ¥æ˜¯å¦å·²å®Œæˆæ‰€æœ‰å¤„ç†"""
        return self.checkpoint.get("processed_count", 0) >= total_count


# ===== æ—¥å¿—å·¥å…· =====
class Logger(object):
    """å°†æ§åˆ¶å°è¾“å‡ºåŒæ—¶å†™å…¥æ–‡ä»¶ã€‚"""

    def __init__(self, filename="default.log"):
        self.terminal = sys.stdout
        self.log = open(filename, "w", encoding="utf-8")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)
        self.log.flush()

    def flush(self):
        pass


# ===== ç¼“å­˜ç®¡ç† =====
class CacheManager:
    """ç®¡ç†æœ¬åœ°ç¼“å­˜"""
    
    def __init__(self, cache_file: str):
        self.cache_file = cache_file
        self.cache = self._load_cache()
    
    def _load_cache(self) -> dict:
        """åŠ è½½ç¼“å­˜"""
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, "r", encoding="utf-8") as f:
                    cache = json.load(f)
                print(f"âœ… åŠ è½½ç¼“å­˜: {self.cache_file} ({len(cache)} æ¡)")
                return cache
            except Exception as e:
                print(f"âš ï¸ åŠ è½½ç¼“å­˜å¤±è´¥: {e}")
                return {}
        return {}
    
    def save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        with open(self.cache_file, "w", encoding="utf-8") as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)
        print(f"âœ… ç¼“å­˜å·²ä¿å­˜: {self.cache_file} ({len(self.cache)} æ¡)")
    
    def get(self, key: str):
        """è·å–ç¼“å­˜"""
        return self.cache.get(key)
    
    def set(self, key: str, value):
        """è®¾ç½®ç¼“å­˜"""
        self.cache[key] = value
    
    def has(self, key: str) -> bool:
        """æ£€æŸ¥ç¼“å­˜æ˜¯å¦å­˜åœ¨"""
        return key in self.cache


# ===== Google Places API (New) =====
class GooglePlacesAPI:
    """Google Places API (New) å°è£…"""
    
    def __init__(self, api_key: str):
        self.api_key = api_key
        self.session = requests.Session()
        self.headers = {
            "Content-Type": "application/json",
            "X-Goog-Api-Key": api_key,
        }
    
    def autocomplete(self, query: str, latitude: float | None = None, longitude: float | None = None) -> dict | None:
        """
        Places API (New) Autocomplete è¯·æ±‚ - è·å– placeId
        
        Args:
            query: æŸ¥è¯¢å­—ç¬¦ä¸²ï¼ˆå­¦æ ¡åç§°ï¼‰
            latitude: çº¬åº¦ï¼ˆç”¨äºä½ç½®åå‘ï¼‰
            longitude: ç»åº¦ï¼ˆç”¨äºä½ç½®åå‘ï¼‰
        
        Returns:
            åŒ…å« placeId çš„å“åº”å­—å…¸ï¼Œæˆ– None å¦‚æœå¤±è´¥
        """
        url = f"{PLACES_API_NEW}/places:autocomplete"
        
        payload: dict = {
            "input": query,
        }
        
        # æ·»åŠ ä½ç½®åå‘ï¼ˆå¦‚æœæä¾›äº†åæ ‡ï¼‰
        if latitude is not None and longitude is not None:
            payload["locationBias"] = {
                "circle": {
                    "center": {
                        "latitude": latitude,
                        "longitude": longitude,
                    },
                    "radius": 50000.0  # 50km æœç´¢åŠå¾„
                }
            }
        
        headers = self.headers.copy()
        # ä½¿ç”¨æ­£ç¡®çš„ FieldMask æ ¼å¼ï¼ˆä¸éœ€è¦æŒ‡å®š placePrediction è·¯å¾„ï¼‰
        headers["X-Goog-FieldMask"] = "suggestions"
        
        for attempt in range(MAX_RETRIES):
            try:
                response = self.session.post(url, json=payload, headers=headers, timeout=15)
                response.raise_for_status()
                
                result = response.json()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰é¢„æµ‹ç»“æœ
                suggestions = result.get("suggestions", [])
                if suggestions:
                    first_suggestion = suggestions[0]
                    place_prediction = first_suggestion.get("placePrediction", {})
                    place_id = place_prediction.get("placeId")
                    
                    if place_id:
                        return {
                            "placeId": place_id,
                            "displayName": place_prediction.get("displayName", "")
                        }
                
                return None
                    
            except requests.exceptions.Timeout:
                print(f"  - è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{MAX_RETRIES}ï¼‰")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(2 ** attempt)
            except requests.exceptions.RequestException as e:
                # æ·»åŠ æ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯ç”¨äºè°ƒè¯•
                try:
                    if hasattr(e, 'response') and e.response is not None:
                        error_detail = e.response.json()
                        print(f"  - API é”™è¯¯: {error_detail.get('error', {}).get('message', str(e))}")
                    else:
                        print(f"  - è¯·æ±‚å¤±è´¥: {str(e)}")
                except:
                    print(f"  - è¯·æ±‚å¤±è´¥: {str(e)}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(2 ** attempt)
        
        return None
    
    def place_details(self, place_id: str) -> dict | None:
        """
        Places API (New) Place Details è¯·æ±‚ - è·å–è¯¦ç»†ä¿¡æ¯
        
        Args:
            place_id: Google Maps place_id
        
        Returns:
            åŒ…å«åœ°ç‚¹è¯¦ç»†ä¿¡æ¯çš„å­—å…¸ï¼Œæˆ– None å¦‚æœå¤±è´¥
        """
        url = f"{PLACES_API_NEW}/places/{place_id}"
        
        params = {
            "fields": "id,displayName,formattedAddress,websiteUri,location,addressComponents"
        }
        
        for attempt in range(MAX_RETRIES):
            try:
                response = self.session.get(url, params=params, headers=self.headers, timeout=15)
                response.raise_for_status()
                
                result = response.json()
                
                # æå–å›½å®¶ä¿¡æ¯
                country = ""
                address_components = result.get("addressComponents", [])
                for component in address_components:
                    if "country" in component.get("types", []):
                        country = component.get("longText", "")
                        break
                
                return {
                    "id": result.get("id", place_id),
                    "displayName": result.get("displayName", {}).get("text", ""),
                    "formattedAddress": result.get("formattedAddress", ""),
                    "websiteUri": result.get("websiteUri", ""),
                    "location": result.get("location", {}),
                    "country": country,
                }
                    
            except requests.exceptions.Timeout:
                print(f"  - è¯·æ±‚è¶…æ—¶ï¼ˆå°è¯• {attempt + 1}/{MAX_RETRIES}ï¼‰")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(2 ** attempt)
            except requests.exceptions.RequestException as e:
                print(f"  - è¯·æ±‚å¤±è´¥: {str(e)}")
                if attempt < MAX_RETRIES - 1:
                    time.sleep(2 ** attempt)
        
        return None


# ===== æ•°æ®å¤„ç† =====
def read_rankings_csv(csv_file: str) -> list[dict]:
    """
    è¯»å–æ’åCSVæ–‡ä»¶
    
    Args:
        csv_file: CSVæ–‡ä»¶è·¯å¾„
    
    Returns:
        å¤§å­¦åˆ—è¡¨
    """
    universities = []
    
    try:
        with open(csv_file, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)
            for row in reader:
                universities.append(row)
        
        print(f"âœ… å·²è¯»å– {len(universities)} æ‰€å¤§å­¦")
        return universities
    except Exception as e:
        print(f"âŒ è¯»å–CSVå¤±è´¥: {e}")
        return []


def query_university(api: GooglePlacesAPI, cache: CacheManager, 
                     university: dict, country: str | None = None) -> dict | None:
    """
    æŸ¥è¯¢å•ä¸ªå¤§å­¦çš„Google Placesä¿¡æ¯
    
    ä½¿ç”¨ä¸¤æ­¥æµç¨‹ï¼š
    1. Autocomplete è·å– placeId
    2. Place Details è·å–è¯¦ç»†ä¿¡æ¯
    
    Args:
        api: GooglePlacesAPIå®ä¾‹
        cache: CacheManagerå®ä¾‹
        university: å¤§å­¦æ•°æ®å­—å…¸ï¼ˆåŒ…å« Name, Country, æ’åç­‰å­—æ®µï¼‰
        country: å›½å®¶ä»£ç ï¼ˆç”¨äºé™åˆ¶æœç´¢ï¼‰
    
    Returns:
        æŸ¥è¯¢ç»“æœï¼ˆåŒ…å« CSV åŸå§‹æ•°æ®çš„å¼•ç”¨ï¼‰
    """
    name = university.get("Name", "").strip()
    if not name:
        return None
    
    # æ£€æŸ¥ç¼“å­˜
    cache_key = name
    if cache.has(cache_key):
        cached = cache.get(cache_key)
        if cached:
            print(f"  âœ“ {name} (æ¥è‡ªç¼“å­˜)")
        return cached
    
    # ç¬¬ä¸€æ­¥ï¼šä½¿ç”¨ Autocomplete è·å– placeId
    print(f"  æŸ¥è¯¢: {name}")
    time.sleep(REQUEST_DELAY)
    
    # è·å–åæ ‡ç”¨äºä½ç½®åå‘ï¼ˆå¦‚æœæœ‰ï¼‰
    latitude = None
    longitude = None
    try:
        lat_str = university.get("Latitude", "")
        lon_str = university.get("Longitude", "")
        if lat_str and lon_str:
            latitude = float(lat_str)
            longitude = float(lon_str)
    except (ValueError, TypeError):
        pass
    
    # æ„å»ºæŸ¥è¯¢å­—ç¬¦ä¸²ï¼šæ ¼å¼ä¸º "University Name, Country"
    if country:
        query_string = f"{name}, {country}"
    else:
        query_string = name
    
    # è°ƒç”¨ Autocomplete API
    autocomplete_result = api.autocomplete(query_string, latitude, longitude)
    
    if not autocomplete_result:
        print(f"    âœ— æœªæ‰¾åˆ°ç»“æœ (Autocomplete)")
        cache.set(cache_key, None)
        return None
    
    place_id = autocomplete_result.get("placeId")
    if not place_id:
        print(f"    âœ— æœªè·å–åˆ° placeId")
        cache.set(cache_key, None)
        return None
    
    # ç¬¬äºŒæ­¥ï¼šä½¿ç”¨ Place Details è·å–è¯¦ç»†ä¿¡æ¯
    time.sleep(REQUEST_DELAY)
    details_result = api.place_details(place_id)
    
    if not details_result:
        print(f"    âœ— è·å–è¯¦ç»†ä¿¡æ¯å¤±è´¥")
        cache.set(cache_key, None)
        return None
    
    # æ„å»ºå®Œæ•´ç»“æœï¼ˆåŒ…å« CSV åŸå§‹æ•°æ®å¼•ç”¨ï¼‰
    place_result = {
        "place_id": place_id,
        "id": details_result.get("id", place_id),
        "csv_name": name,  # ä¿å­˜ CSV åŸå§‹åç§°
        "api_name": details_result.get("displayName", name),  # API è¿”å›çš„åç§°
        "formatted_address": details_result.get("formattedAddress", ""),
        "website": details_result.get("websiteUri", ""),
        "location": details_result.get("location", {}),
        "country": details_result.get("country", country or ""),
        "csv_data": university,  # ä¿å­˜æ•´ä¸ª CSV è¡Œæ•°æ®
    }
    
    print(f"    âœ“ æ‰¾åˆ°: {place_result['api_name']}")
    
    # ç¼“å­˜ç»“æœ
    cache.set(cache_key, place_result)
    
    return place_result


def deduplicate_by_place_id(results: list[dict]) -> dict[str, dict]:
    """
    æŒ‰place_idå»é‡ï¼Œå¹¶åˆå¹¶æ’åä¿¡æ¯
    
    Args:
        results: æŸ¥è¯¢ç»“æœåˆ—è¡¨
    
    Returns:
        æŒ‰place_idåˆ†ç»„çš„å»é‡ç»“æœ
    """
    deduplicated = {}
    
    for result in results:
        if not result:
            continue
        
        place_id = result.get("place_id")
        if not place_id:
            continue
        
        if place_id not in deduplicated:
            deduplicated[place_id] = {
                "place_id": place_id,
                "csv_data_list": [],  # ä¿å­˜æ‰€æœ‰åŒ¹é…çš„ CSV æ•°æ®ï¼ˆç”¨äºåˆå¹¶æ’åï¼‰
                "csv_names": set(),  # CSV ä¸­ä¸åŒçš„åç§°
                "locations": set(),  # ä¸åŒä½ç½®
                "original_data": result,  # ä¿å­˜ç¬¬ä¸€ä¸ªåŒ¹é…çš„å®Œæ•´æ•°æ®
                "count": 0,  # å‡ºç°æ¬¡æ•°
            }
        
        # ä¿å­˜ CSV æ•°æ®ç”¨äºåç»­æ’ååˆå¹¶
        csv_data = result.get("csv_data", {})
        if csv_data:
            deduplicated[place_id]["csv_data_list"].append(csv_data)
        
        # ç´¯è®¡ CSV åç§°ä¿¡æ¯
        csv_name = result.get("csv_name")
        if csv_name:
            deduplicated[place_id]["csv_names"].add(csv_name)
        
        # åœ°ç†ä½ç½® - ä»æ–° API çš„ location å­—æ®µæå–
        location = result.get("location", {})
        if location:
            lat = location.get("latitude")
            lng = location.get("longitude")
            if lat and lng:
                deduplicated[place_id]["locations"].add((lat, lng))
        
        deduplicated[place_id]["count"] += 1
    
    return deduplicated


def build_output_json(deduplicated: dict, university_data: list[dict]) -> list[dict]:
    """
    æ„å»ºè¾“å‡ºJSONæ ¼å¼
    
    è¾“å‡ºå­—æ®µ: id, name, type, majorCategory, natureOfRunning, QSrank, THErank, LACrank, campuses
    - type: ç»Ÿä¸€ä¸º "Undergraduate"
    - name: æ¥è‡ª CSV çš„åŸå§‹åç§°
    - majorCategory: æ ¹æ®æ˜¯å¦æœ‰ LAC æ’åç¡®å®š (LAC / Comprehensive)
    - QSrank, THErank, LACrank: æ¥è‡ª CSV çš„æ’åï¼ˆå¯èƒ½ä¸º nullï¼‰
    - campuses: åªåŒ…å«ä¸€ä¸ªå¯¹è±¡ï¼Œå­—æ®µä¸º id, name, address, country, website, location
    
    Args:
        deduplicated: å»é‡åçš„ç»“æœ
        university_data: åŸå§‹å¤§å­¦æ•°æ®
    
    Returns:
        æ ‡å‡†è¾“å‡ºæ ¼å¼çš„åˆ—è¡¨
    """
    output = []
    
    # æ„å»ºåŸå§‹æ•°æ®çš„æ˜ å°„ï¼ˆç”¨äºè·å–æ’åç­‰ï¼‰
    data_map = {}
    for univ in university_data:
        name = univ.get("Name", "").strip()
        if name:
            data_map[name] = univ
    
    # å¤„ç†æ¯ä¸ªå»é‡åçš„åœ°ç‚¹
    for place_id, dedup_info in deduplicated.items():
        original_data = dedup_info["original_data"]
        csv_data_list = dedup_info.get("csv_data_list", [])
        csv_name = original_data.get("csv_name", "Unknown University")
        
        # åˆå¹¶æ’åä¿¡æ¯ï¼šä»æ‰€æœ‰åŒ¹é…çš„å¤§å­¦ä¸­æ”¶é›†æ’å
        # ä¼˜å…ˆé€‰æ‹©æœ‰æ’åçš„æ•°æ®ï¼ˆè€Œä¸æ˜¯åªç”¨ç¬¬ä¸€æ¡ï¼‰
        qs_rank: int | str | None = None
        the_rank: int | str | None = None
        lac_rank: int | str | None = None
        
        # ä» csv_data_list ä¸­æ‰¾ç¬¬ä¸€ä¸ªæœ‰ QS_Rank çš„
        for csv_data in csv_data_list:
            qs_rank_str = csv_data.get("QS_Rank", "").strip()
            if qs_rank_str and qs_rank_str != "":
                try:
                    qs_rank = int(qs_rank_str)
                except ValueError:
                    qs_rank = qs_rank_str
                break
        
        # ä» csv_data_list ä¸­æ‰¾ç¬¬ä¸€ä¸ªæœ‰ THE_Rank çš„
        for csv_data in csv_data_list:
            the_rank_str = csv_data.get("THE_Rank", "").strip()
            if the_rank_str and the_rank_str != "":
                try:
                    the_rank = int(float(the_rank_str))
                except ValueError:
                    the_rank = the_rank_str
                break
        
        # ä» csv_data_list ä¸­æ‰¾ç¬¬ä¸€ä¸ªæœ‰ USNews_Rank çš„
        for csv_data in csv_data_list:
            lac_rank_str = csv_data.get("USNews_Rank", "").strip()
            if lac_rank_str and lac_rank_str != "":
                try:
                    lac_rank = int(lac_rank_str)
                except ValueError:
                    lac_rank = lac_rank_str
                break
        
        # å¦‚æœæ²¡æœ‰åœ¨ csv_data_list ä¸­æ‰¾åˆ°ä»»ä½•æ’åï¼Œåˆ™ä½¿ç”¨åŸå§‹æ•°æ®çš„æ’å
        if csv_data_list and (qs_rank is None or the_rank is None or lac_rank is None):
            # è¿™é€šå¸¸ä¸ä¼šå‘ç”Ÿï¼Œå› ä¸ºæˆ‘ä»¬å·²ç»éå†äº†æ‰€æœ‰æ•°æ®
            pass
        
        # æ ¹æ®æ˜¯å¦æœ‰ LAC æ’åç¡®å®š majorCategory
        major_category = "LAC" if lac_rank is not None else "Comprehensive"
        
        # è·å– natureOfRunningï¼Œä¼˜å…ˆä» csv_data_list ä¸­é€‰æ‹©
        nature_of_running = "Public"
        for csv_data in csv_data_list:
            if csv_data.get("natureOfRunning"):
                nature_of_running = csv_data.get("natureOfRunning")
                break
        
        # åªä¿ç•™ä¸€ä¸ªcampusï¼ŒåŒ…å«idã€addressã€countryã€websiteç­‰å­—æ®µ
        campuses = []
        
        # ä¼˜å…ˆä½¿ç”¨ç¬¬ä¸€ä¸ªä½ç½®
        if dedup_info["locations"]:
            lat, lng = list(dedup_info["locations"])[0]
            campus_obj = {
                "id": place_id,  # Google Maps place_id
                "name": None,    # campus åç§°ä¸º null
                "address": original_data.get("formatted_address", ""),
                "country": original_data.get("country", ""),  # ä» API å“åº”è·å–
                "website": original_data.get("website", ""),
                "location": {
                    "type": "Point",
                    "coordinates": [lng, lat]  # GeoJSONæ ¼å¼ï¼š[longitude, latitude]
                }
            }
            campuses.append(campus_obj)
        elif original_data.get("location"):
            # æ–° API æ ¼å¼çš„ä½ç½®æ•°æ®
            location = original_data.get("location", {})
            lat = location.get("latitude")
            lng = location.get("longitude")
            if lat and lng:
                campus_obj = {
                    "id": place_id,
                    "name": None,
                    "address": original_data.get("formatted_address", ""),
                    "country": original_data.get("country", ""),
                    "website": original_data.get("website", ""),
                    "location": {
                        "type": "Point",
                        "coordinates": [lng, lat]
                    }
                }
                campuses.append(campus_obj)
        
        # æ„å»ºè¾“å‡ºé¡¹
        output_item = {
            "id": place_id,  # ä½¿ç”¨ Google Maps place_id ä½œä¸º id
            "name": csv_name,  # ä½¿ç”¨ CSV ä¸­çš„åŸå§‹åç§°
            "type": "Undergraduate",  # ç»Ÿä¸€ä¸º Undergraduate
            "majorCategory": major_category,  # LAC æˆ– Comprehensive
            "natureOfRunning": nature_of_running,  # ä» CSV è·å–
            "QSrank": qs_rank,
            "THErank": the_rank,
            "LACrank": lac_rank,
            "campuses": campuses,
        }
        
        output.append(output_item)
    
    return output


def main():
    """ä¸»ç¨‹åº - æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    # è®¾ç½®æ—¥å¿—
    log_filename = f"{LOG_FILE_PREFIX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    sys.stdout = Logger(log_filename)
    
    print(f"æ—¥å¿—å°†è®°å½•åˆ°æ–‡ä»¶: {log_filename}")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not GOOGLE_MAPS_KEY:
        print("âŒ é”™è¯¯: æœªè®¾ç½® GOOGLE_MAPS_KEY ç¯å¢ƒå˜é‡")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: GOOGLE_MAPS_KEY=your_api_key")
        return
    
    # 1. è¯»å–CSV
    print("\n[1/6] è¯»å–æ’åCSV...")
    universities = read_rankings_csv(RANKINGS_CSV)
    if not universities:
        print("âŒ æ— æ³•è¯»å–å¤§å­¦æ•°æ®")
        return
    
    # 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹å’Œç¼“å­˜
    print("\n[2/6] åŠ è½½æ£€æŸ¥ç‚¹å’Œç¼“å­˜...")
    checkpoint = CheckpointManager(CHECKPOINT_FILE)
    cache = CacheManager(CACHE_FILE)
    api = GooglePlacesAPI(GOOGLE_MAPS_KEY)
    
    # è·å–å·²å¤„ç†çš„å¤§å­¦åç§°
    processed_names = checkpoint.get_processed_names()
    failed_names = checkpoint.get_failed_names()
    
    print(f"  - æ€»å¤§å­¦æ•°: {len(universities)}")
    print(f"  - å·²å¤„ç†: {len(processed_names)}")
    print(f"  - å¤±è´¥: {len(failed_names)}")
    print(f"  - å¾…å¤„ç†: {len(universities) - len(processed_names)}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    if checkpoint.is_completed(len(universities)):
        print("\nâœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†ï¼Œè·³è½¬åˆ°å»é‡æ­¥éª¤...")
    else:
        # 3. ç»§ç»­æ‰¹é‡æŸ¥è¯¢
        print(f"\n[3/6] æŸ¥è¯¢Google Places API...")
        print(f"ç»§ç»­å¤„ç†å‰©ä½™ {len(universities) - len(processed_names)} æ‰€å¤§å­¦")
        
        all_results = []
        success_count = checkpoint.checkpoint.get("success_count", 0)
        failed_count = checkpoint.checkpoint.get("failed_count", 0)
        processed_list = list(processed_names)
        failed_list = list(failed_names)
        
        try:
            for idx, university in enumerate(universities, 1):
                name = university.get("Name", "").strip()
                
                # è·³è¿‡å·²å¤„ç†çš„å¤§å­¦
                if name in processed_names:
                    # å¦‚æœåœ¨ç¼“å­˜ä¸­ï¼ŒåŠ è½½ç»“æœ
                    cached = cache.get(name)
                    if cached:
                        all_results.append(cached)
                    continue
                
                print(f"\n  [{idx}/{len(universities)}]", end=" ")
                
                country = university.get("Country", "")
                result = query_university(api, cache, university, country)
                
                if result:
                    all_results.append(result)
                    success_count += 1
                    processed_list.append(name)
                else:
                    failed_count += 1
                    failed_list.append(name)
                    processed_list.append(name)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯10æ¡æˆ–å¤„ç†å®Œæˆæ—¶ï¼‰
                if idx % 10 == 0:
                    print(f"\n  å·²å¤„ç† {idx}/{len(universities)} æ¡")
                    cache.save_cache()
                    checkpoint.save_checkpoint(idx, success_count, failed_count, 
                                              processed_list, failed_list)
                    print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        
        except KeyboardInterrupt:
            print("\n\nâš ï¸ å¤„ç†è¢«ä¸­æ–­!")
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šç»§ç»­...")
            cache.save_cache()
            checkpoint.save_checkpoint(len(processed_list), success_count, failed_count,
                                      processed_list, failed_list)
            return
        except Exception as e:
            print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šç»§ç»­...")
            cache.save_cache()
            checkpoint.save_checkpoint(len(processed_list), success_count, failed_count,
                                      processed_list, failed_list)
            raise
        
        # æœ€åä¿å­˜ä¸€æ¬¡ç¼“å­˜å’Œæ£€æŸ¥ç‚¹
        cache.save_cache()
        checkpoint.save_checkpoint(len(universities), success_count, failed_count,
                                  processed_list, failed_list)
    
    # 4. é‡æ–°åŠ è½½æ‰€æœ‰ç¼“å­˜çš„ç»“æœç”¨äºå»é‡
    print("\n[4/6] é‡æ–°åŠ è½½æ‰€æœ‰ç¼“å­˜ç»“æœ...")
    all_results = []
    for university in universities:
        name = university.get("Name", "").strip()
        cached = cache.get(name)
        if cached:
            all_results.append(cached)
    
    print(f"âœ… å·²åŠ è½½ {len(all_results)} æ¡ç¼“å­˜ç»“æœ")
    
    # 5. å»é‡
    print("\n[5/6] æŒ‰place_idå»é‡...")
    deduplicated = deduplicate_by_place_id(all_results)
    print(f"âœ… åŸå§‹ç»“æœ: {len(all_results)} æ¡")
    print(f"âœ… å»é‡å: {len(deduplicated)} æ¡")
    
    # ç»Ÿè®¡é‡å¤æƒ…å†µ
    duplicates = {pid: info for pid, info in deduplicated.items() if info["count"] > 1}
    if duplicates:
        print(f"âš ï¸ å‘ç° {len(duplicates)} ä¸ªé‡å¤åœ°ç‚¹:")
        for place_id, info in list(duplicates.items())[:5]:
            print(f"   - {list(info['csv_names'])[0]}: {info['count']} æ¬¡")
    
    # 6. è¾“å‡ºç»“æœ
    print("\n[6/6] ç”Ÿæˆè¾“å‡ºJSON...")
    output = build_output_json(deduplicated, universities)
    
    with open(OUTPUT_JSON_FILE, "w", encoding="utf-8") as f:
        json.dump(output, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… ç»“æœå·²ä¿å­˜: {OUTPUT_JSON_FILE} ({len(output)} æ¡)")
    
    # 7. ç”Ÿæˆè¾…åŠ©è¾“å‡ºæ–‡ä»¶
    print("\n[7/7] ç”Ÿæˆè¾…åŠ©æ–‡ä»¶...")
    
    # æ”¶é›†è¢«æ‹’ç»çš„å¤§å­¦ï¼ˆç¼“å­˜ä¸­ä¸º None çš„ï¼‰
    rejected_universities = []
    no_website_universities = []
    
    for university in universities:
        name = university.get("Name", "").strip()
        cached = cache.get(name)
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹’ç»ï¼ˆæŸ¥è¯¢å¤±è´¥ï¼‰
        if cached is None:
            rejected_universities.append(university)
        # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰ç½‘ç«™
        elif cached and not cached.get("website"):
            no_website_universities.append(university)
    
    # è¾“å‡ºè¢«æ‹’ç»çš„å¤§å­¦
    if rejected_universities:
        rejected_csv_file = "rejected_pois_global.csv"
        try:
            with open(rejected_csv_file, "w", newline="", encoding="utf-8") as f:
                if rejected_universities:
                    writer = csv.DictWriter(f, fieldnames=rejected_universities[0].keys())
                    writer.writeheader()
                    writer.writerows(rejected_universities)
            print(f"âœ… è¢«æ‹’ç»çš„å¤§å­¦å·²ä¿å­˜: {rejected_csv_file} ({len(rejected_universities)} æ¡)")
        except Exception as e:
            print(f"âŒ ä¿å­˜è¢«æ‹’ç»å¤§å­¦å¤±è´¥: {e}")
    
    # è¾“å‡ºæ²¡æœ‰ç½‘ç«™çš„å¤§å­¦
    if no_website_universities:
        no_website_csv_file = "universities_with_no_website.csv"
        try:
            with open(no_website_csv_file, "w", newline="", encoding="utf-8") as f:
                if no_website_universities:
                    writer = csv.DictWriter(f, fieldnames=no_website_universities[0].keys())
                    writer.writeheader()
                    writer.writerows(no_website_universities)
            print(f"âœ… æ²¡æœ‰ç½‘ç«™çš„å¤§å­¦å·²ä¿å­˜: {no_website_csv_file} ({len(no_website_universities)} æ¡)")
        except Exception as e:
            print(f"âŒ ä¿å­˜æ²¡æœ‰ç½‘ç«™å¤§å­¦å¤±è´¥: {e}")
    
    print("\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆ!")
    print(f"  - è¾“å…¥: {len(universities)} æ‰€å¤§å­¦")
    print(f"  - æˆåŠŸæŸ¥è¯¢: {len(all_results)} æ¡")
    print(f"  - æŸ¥è¯¢å¤±è´¥ï¼ˆè¢«æ‹’ç»ï¼‰: {len(rejected_universities)} æ¡")
    print(f"  - æ²¡æœ‰ç½‘ç«™: {len(no_website_universities)} æ¡")
    print(f"  - å»é‡å: {len(output)} æ¡")
    if len(all_results) > 0:
        print(f"  - å»é‡ç‡: {(1 - len(output)/len(all_results))*100:.1f}%")
    print("\nğŸ’¡ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {OUTPUT_JSON_FILE} - æœ€ç»ˆè¾“å‡º")
    print(f"  - rejected_pois_global.csv - æŸ¥è¯¢å¤±è´¥çš„å¤§å­¦")
    print(f"  - universities_with_no_website.csv - æ²¡æœ‰ç½‘ç«™çš„å¤§å­¦")


if __name__ == "__main__":
    main()
