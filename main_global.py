"""
Global University Geocoder - Main Entry Point

Batch geocoding of global universities using Google Places API (New).
Supports checkpoint resumption and local caching for efficient processing.
"""

import sys
import json
import csv
import time
from datetime import datetime

from src.config_global import (
    RANKINGS_CSV,
    CACHE_FILE,
    OUTPUT_JSON_FILE,
    CHECKPOINT_FILE,
    LOG_FILE_PREFIX,
    GOOGLE_MAPS_KEY,
    PLACES_API_NEW,
    MAX_RETRIES,
    REQUEST_DELAY,
    REJECTED_POIS_CSV,
    NO_WEBSITE_CSV,
)
from src.api.google_places import GooglePlacesAPI, CacheManager
from src.data.loader_global import read_rankings_csv
from src.utils.checkpoint import Logger, CheckpointManager
from src.processors.query_processor import query_university, deduplicate_by_place_id
from src.output.writer_global import (
    build_output_json,
    write_output_json,
    write_rejected_universities_csv,
    write_no_website_universities_csv,
)


def main():
    """ä¸»ç¨‹åº - æ”¯æŒæ–­ç‚¹ç»­ä¼ """
    # è®¾ç½®æ—¥å¿—
    log_filename = f"{LOG_FILE_PREFIX}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    sys.stdout = Logger(log_filename)
    
    print(f"æ—¥å¿—å°†è®°å½•åˆ°æ–‡ä»¶: {log_filename}")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not GOOGLE_MAPS_KEY:
        print("âŒ é”™è¯¯: æœªè®¾ç½® GOOGLE_MAPS_KEY ç¯å¢ƒå˜é‡")
        print("è¯·åœ¨ .env æ–‡ä»¶ä¸­è®¾ç½®: GOOGLE_MAPS_KEY=your_api_key")
        return
    
    # 1. è¯»å–CSV
    print("\n[1/6] è¯»å–æ’åCSV...")
    universities = read_rankings_csv(RANKINGS_CSV)
    if not universities:
        print("âŒ æ— æ³•è¯»å–å¤§å­¦æ•°æ®")
        return
    
    # 2. åˆå§‹åŒ–æ£€æŸ¥ç‚¹å’Œç¼“å­˜
    print("\n[2/6] åŠ è½½æ£€æŸ¥ç‚¹å’Œç¼“å­˜...")
    checkpoint = CheckpointManager(CHECKPOINT_FILE)
    cache = CacheManager(CACHE_FILE)
    api = GooglePlacesAPI(GOOGLE_MAPS_KEY, PLACES_API_NEW, MAX_RETRIES, REQUEST_DELAY)
    
    # è·å–å·²å¤„ç†çš„å¤§å­¦åç§°
    processed_names = checkpoint.get_processed_names()
    failed_names = checkpoint.get_failed_names()
    
    print(f"  - æ€»å¤§å­¦æ•°: {len(universities)}")
    print(f"  - å·²å¤„ç†: {len(processed_names)}")
    print(f"  - å¤±è´¥: {len(failed_names)}")
    print(f"  - å¾…å¤„ç†: {len(universities) - len(processed_names)}")
    
    # æ£€æŸ¥æ˜¯å¦å·²å®Œæˆ
    if checkpoint.is_completed(len(universities)):
        print("\nâœ… æ‰€æœ‰æ•°æ®å·²å¤„ç†ï¼Œè·³è½¬åˆ°å»é‡æ­¥éª¤...")
    else:
        # 3. ç»§ç»­æ‰¹é‡æŸ¥è¯¢
        print(f"\n[3/6] æŸ¥è¯¢Google Places API...")
        print(f"ç»§ç»­å¤„ç†å‰©ä½™ {len(universities) - len(processed_names)} æ‰€å¤§å­¦")
        
        all_results = []
        success_count = checkpoint.checkpoint.get("success_count", 0)
        failed_count = checkpoint.checkpoint.get("failed_count", 0)
        processed_list = list(processed_names)
        failed_list = list(failed_names)
        
        try:
            for idx, university in enumerate(universities, 1):
                name = university.get("Name", "").strip()
                
                # è·³è¿‡å·²å¤„ç†çš„å¤§å­¦
                if name in processed_names:
                    # å¦‚æœåœ¨ç¼“å­˜ä¸­ï¼ŒåŠ è½½ç»“æœ
                    cached = cache.get(name)
                    if cached:
                        all_results.append(cached)
                    continue
                
                print(f"\n  [{idx}/{len(universities)}]", end=" ")
                
                country = university.get("Country", "")
                result = query_university(api, cache, university, country)
                
                if result:
                    all_results.append(result)
                    success_count += 1
                    processed_list.append(name)
                else:
                    failed_count += 1
                    failed_list.append(name)
                    processed_list.append(name)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆæ¯10æ¡æˆ–å¤„ç†å®Œæˆæ—¶ï¼‰
                if idx % 10 == 0:
                    print(f"\n  å·²å¤„ç† {idx}/{len(universities)} æ¡")
                    cache.save_cache()
                    checkpoint.save_checkpoint(idx, success_count, failed_count, 
                                              processed_list, failed_list)
                    print(f"  ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜")
        
        except KeyboardInterrupt:
            print("\n\nâš ï¸ å¤„ç†è¢«ä¸­æ–­!")
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šç»§ç»­...")
            cache.save_cache()
            checkpoint.save_checkpoint(len(processed_list), success_count, failed_count,
                                      processed_list, failed_list)
            return
        except Exception as e:
            print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
            print("âœ… æ£€æŸ¥ç‚¹å·²ä¿å­˜ï¼Œä¸‹æ¬¡è¿è¡Œæ—¶ä¼šç»§ç»­...")
            cache.save_cache()
            checkpoint.save_checkpoint(len(processed_list), success_count, failed_count,
                                      processed_list, failed_list)
            raise
        
        # æœ€åä¿å­˜ä¸€æ¬¡ç¼“å­˜å’Œæ£€æŸ¥ç‚¹
        cache.save_cache()
        checkpoint.save_checkpoint(len(universities), success_count, failed_count,
                                  processed_list, failed_list)
    
    # 4. é‡æ–°åŠ è½½æ‰€æœ‰ç¼“å­˜çš„ç»“æœç”¨äºå»é‡
    print("\n[4/6] é‡æ–°åŠ è½½æ‰€æœ‰ç¼“å­˜ç»“æœ...")
    all_results = []
    for university in universities:
        name = university.get("Name", "").strip()
        cached = cache.get(name)
        if cached:
            all_results.append(cached)
    
    print(f"âœ… å·²åŠ è½½ {len(all_results)} æ¡ç¼“å­˜ç»“æœ")
    
    # 5. å»é‡
    print("\n[5/6] æŒ‰place_idå»é‡...")
    deduplicated = deduplicate_by_place_id(all_results)
    print(f"âœ… åŸå§‹ç»“æœ: {len(all_results)} æ¡")
    print(f"âœ… å»é‡å: {len(deduplicated)} æ¡")
    
    # ç»Ÿè®¡é‡å¤æƒ…å†µ
    duplicates = {pid: info for pid, info in deduplicated.items() if info["count"] > 1}
    if duplicates:
        print(f"âš ï¸ å‘ç° {len(duplicates)} ä¸ªé‡å¤åœ°ç‚¹:")
        for place_id, info in list(duplicates.items())[:5]:
            print(f"   - {list(info['csv_names'])[0]}: {info['count']} æ¬¡")
    
    # 6. è¾“å‡ºç»“æœ
    print("\n[6/6] ç”Ÿæˆè¾“å‡ºJSON...")
    output = build_output_json(deduplicated, universities)
    write_output_json(output, OUTPUT_JSON_FILE)
    
    # 7. ç”Ÿæˆè¾…åŠ©è¾“å‡ºæ–‡ä»¶
    print("\n[7/7] ç”Ÿæˆè¾…åŠ©æ–‡ä»¶...")
    
    # æ”¶é›†è¢«æ‹’ç»çš„å¤§å­¦ï¼ˆç¼“å­˜ä¸­ä¸º None çš„ï¼‰
    rejected_universities = []
    no_website_universities = []
    
    for university in universities:
        name = university.get("Name", "").strip()
        cached = cache.get(name)
        
        # æ£€æŸ¥æ˜¯å¦è¢«æ‹’ç»ï¼ˆæŸ¥è¯¢å¤±è´¥ï¼‰
        if cached is None:
            rejected_universities.append(university)
        # æ£€æŸ¥æ˜¯å¦æ²¡æœ‰ç½‘ç«™
        elif cached and not cached.get("website"):
            no_website_universities.append(university)
    
    # è¾“å‡ºè¢«æ‹’ç»çš„å¤§å­¦
    write_rejected_universities_csv(rejected_universities, REJECTED_POIS_CSV)
    
    # è¾“å‡ºæ²¡æœ‰ç½‘ç«™çš„å¤§å­¦
    write_no_website_universities_csv(no_website_universities, NO_WEBSITE_CSV)
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("âœ… å¤„ç†å®Œæˆ!")
    print(f"  - è¾“å…¥: {len(universities)} æ‰€å¤§å­¦")
    print(f"  - æˆåŠŸæŸ¥è¯¢: {len(all_results)} æ¡")
    print(f"  - æŸ¥è¯¢å¤±è´¥ï¼ˆè¢«æ‹’ç»ï¼‰: {len(rejected_universities)} æ¡")
    print(f"  - æ²¡æœ‰ç½‘ç«™: {len(no_website_universities)} æ¡")
    print(f"  - å»é‡å: {len(output)} æ¡")
    if len(all_results) > 0:
        print(f"  - å»é‡ç‡: {(1 - len(output)/len(all_results))*100:.1f}%")
    print("\nğŸ’¡ è¾“å‡ºæ–‡ä»¶:")
    print(f"  - {OUTPUT_JSON_FILE} - æœ€ç»ˆè¾“å‡º")
    print(f"  - {REJECTED_POIS_CSV} - æŸ¥è¯¢å¤±è´¥çš„å¤§å­¦")
    print(f"  - {NO_WEBSITE_CSV} - æ²¡æœ‰ç½‘ç«™çš„å¤§å­¦")


if __name__ == "__main__":
    main()
